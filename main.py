#!/usr/bin/env python3
"""
Enhanced Ballast Quality-Focused Point Cloud Reduction System v2.4.0 (Adaptive Target Finding)

NEW IN v2.4.0 - ADAPTIVE TARGET FINDING:
- Automatic optimal target point detection when initial target fails
- Multi-strategy target adaptation based on model complexity
- Reconstruction-aware target adjustment with fallback mechanisms
- Progressive target scaling with quality validation
- Smart retry logic with complexity-based target ranges

SPECIALIZED FOR BALLAST QUALITY + ENHANCED DETAIL PRESERVATION + ADAPTIVE TARGETING:
- Multi-scale surface analysis for better detail detection
- Zone-based adaptive processing (high/medium/low detail areas)
- Enhanced texture-preserving reconstruction methods
- Much more conservative point reduction with detail zones
- Better feature preservation for rough surfaces
- Alternative reconstruction methods that preserve texture
- NEW: Automatic target finding when user target fails
- Respects user targets while maximizing quality and ensuring success

Usage (SAME COMMANDS):
    python ballast-quality-focused-v2.4.0.py /home/railcmu/Desktop/BPK/ballast --count 100 --workers 2
    python ballast-quality-focused-v2.4.0.py /path/to/models --ratio 0.3 --workers 8 --adaptive-target

Key NEW Features in v2.4.0:
    âœ… Adaptive target finding when initial target fails
    âœ… Multi-strategy target adaptation (conservative, moderate, aggressive)
    âœ… Reconstruction-aware target adjustment
    âœ… Progressive target scaling with quality validation
    âœ… Smart retry logic with complexity-based ranges
    âœ… Automatic fallback to safe targets for successful reconstruction
    âœ… Target optimization based on surface complexity analysis
    âœ… Quality-first target selection with success guarantees

Requirements:
    pip install numpy pandas scikit-learn trimesh open3d scipy

Author: theeradon  
Version: 2.4.0 (Adaptive Target Finding + Enhanced Detail-Preserving)
"""

import numpy as np
import pandas as pd
import os
import sys
import time
import argparse
import logging
import warnings
from pathlib import Path
from typing import Tuple, List, Dict, Optional, Union
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import multiprocessing as mp
import math
from functools import wraps

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Try importing required libraries with helpful error messages
try:
    import trimesh
except ImportError:
    print("Error: trimesh not installed. Run: pip install trimesh")
    sys.exit(1)

try:
    import open3d as o3d
except ImportError:
    print("Error: open3d not installed. Run: pip install open3d")
    sys.exit(1)

try:
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.cluster import DBSCAN
    from sklearn.neighbors import NearestNeighbors
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import ParameterGrid
except ImportError:
    print("Error: scikit-learn not installed. Run: pip install scikit-learn")
    sys.exit(1)

# Optional scipy import for enhanced boundary detection
try:
    from scipy.spatial import ConvexHull
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False


def setup_logging(verbose: bool = False, log_file: Optional[str] = None):
    """Setup logging configuration with file and console output"""
    
    if log_file:
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
    
    level = logging.DEBUG if verbose else logging.INFO
    
    detailed_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    simple_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    
    root_logger = logging.getLogger()
    root_logger.setLevel(level)
    
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(level)
    console_handler.setFormatter(simple_formatter)
    root_logger.addHandler(console_handler)
    
    if log_file:
        file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(detailed_formatter)
        root_logger.addHandler(file_handler)
        
        logging.info(f"ðŸ“ Logging to file: {log_file}")
        logging.info(f"ðŸ“º Console logging level: {logging.getLevelName(level)}")
        
    return root_logger


logger = logging.getLogger(__name__)


def time_function(func):
    """Decorator to time function execution"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        start = time.time()
        result = func(self, *args, **kwargs)
        elapsed = time.time() - start
        logger.debug(f"â±ï¸ {func.__name__}: {elapsed:.2f}s")
        return result
    return wrapper


class MeshSmoothingAndRepair:
    """
    NEW: Mesh Smoothing and Hole Repair System for perfect smooth output meshes
    """
    
    def __init__(self):
        self.smoothing_methods = {
            'laplacian': {
                'iterations': [1, 3, 5, 10],
                'description': 'Laplacian smoothing - preserves shape while smoothing'
            },
            'taubin': {
                'iterations': [1, 3, 5, 10], 
                'description': 'Taubin smoothing - reduces shrinkage'
            },
            'simple': {
                'iterations': [1, 2, 3, 5],
                'description': 'Simple smoothing - fast and effective'
            }
        }
        
        self.hole_filling_strategies = {
            'conservative': {
                'max_hole_size': 100,
                'iterations': 3,
                'description': 'Fill only small holes conservatively'
            },
            'moderate': {
                'max_hole_size': 500,
                'iterations': 5,
                'description': 'Fill medium holes with moderate approach'
            },
            'aggressive': {
                'max_hole_size': 2000,
                'iterations': 10,
                'description': 'Fill large holes aggressively'
            }
        }
    
    def detect_mesh_holes(self, mesh: trimesh.Trimesh) -> Dict:
        """
        Detect holes and quality issues in mesh
        """
        analysis = {
            'has_holes': False,
            'hole_count': 0,
            'hole_sizes': [],
            'is_watertight': False,
            'boundary_edges': 0,
            'quality_score': 0.0
        }
        
        try:
            # Check if mesh is watertight
            analysis['is_watertight'] = mesh.is_watertight
            
            # Get boundary edges (edges that belong to only one face)
            edges = mesh.edges_unique
            edge_faces = mesh.edges_unique_inverse
            boundary_mask = np.bincount(edge_faces) == 1
            boundary_edges = edges[boundary_mask]
            analysis['boundary_edges'] = len(boundary_edges)
            
            # Estimate hole count and sizes based on boundary loops
            if len(boundary_edges) > 0:
                analysis['has_holes'] = True
                # Simple heuristic: group connected boundary edges
                analysis['hole_count'] = max(1, len(boundary_edges) // 10)
                analysis['hole_sizes'] = [len(boundary_edges) // max(1, analysis['hole_count'])]
            
            # Calculate quality score (0-1, where 1 is perfect)
            if analysis['is_watertight']:
                analysis['quality_score'] = 1.0
            elif analysis['boundary_edges'] == 0:
                analysis['quality_score'] = 0.9
            elif analysis['boundary_edges'] < 10:
                analysis['quality_score'] = 0.7
            elif analysis['boundary_edges'] < 50:
                analysis['quality_score'] = 0.5
            else:
                analysis['quality_score'] = 0.3
                
        except Exception as e:
            logger.warning(f"âš ï¸ Hole detection failed: {e}")
            analysis['quality_score'] = 0.5
        
        return analysis
    
    def fill_mesh_holes(self, mesh: trimesh.Trimesh, strategy: str = 'moderate') -> trimesh.Trimesh:
        """
        Fill holes in mesh using Open3D hole filling
        """
        if strategy not in self.hole_filling_strategies:
            strategy = 'moderate'
            
        config = self.hole_filling_strategies[strategy]
        logger.info(f"ðŸ”§ Filling holes using {strategy} strategy...")
        
        try:
            # Convert to Open3D mesh
            o3d_mesh = o3d.geometry.TriangleMesh()
            o3d_mesh.vertices = o3d.utility.Vector3dVector(mesh.vertices)
            o3d_mesh.triangles = o3d.utility.Vector3iVector(mesh.faces)
            
            # Remove duplicated vertices and faces
            o3d_mesh.remove_duplicated_vertices()
            o3d_mesh.remove_duplicated_triangles()
            o3d_mesh.remove_non_manifold_edges()
            
            # Fill holes (approximate method using Poisson reconstruction)
            # First, sample points from the mesh
            point_cloud = o3d_mesh.sample_points_uniformly(number_of_points=len(mesh.vertices))
            point_cloud.estimate_normals()
            
            # Use Poisson reconstruction to fill holes
            filled_mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(
                point_cloud, 
                depth=9,  # Medium detail to fill holes without over-smoothing
                width=0,
                scale=1.1,
                linear_fit=False
            )
            
            # Convert back to trimesh
            if len(np.asarray(filled_mesh.vertices)) > 0:
                vertices = np.asarray(filled_mesh.vertices)
                faces = np.asarray(filled_mesh.triangles)
                repaired_mesh = trimesh.Trimesh(vertices=vertices, faces=faces)
                
                logger.info(f"âœ… Hole filling completed: {len(mesh.faces)} â†’ {len(faces)} faces")
                return repaired_mesh
            else:
                logger.warning("âš ï¸ Hole filling failed, returning original mesh")
                return mesh
                
        except Exception as e:
            logger.warning(f"âš ï¸ Hole filling failed: {e}, returning original mesh")
            return mesh
    
    def smooth_mesh(self, mesh: trimesh.Trimesh, method: str = 'simple', 
                   iterations: int = 3) -> trimesh.Trimesh:
        """
        Smooth mesh to create perfect smooth surfaces
        """
        if method not in self.smoothing_methods:
            method = 'simple'
            
        logger.info(f"ðŸŽ¨ Smoothing mesh using {method} method ({iterations} iterations)...")
        
        try:
            # Convert to Open3D mesh
            o3d_mesh = o3d.geometry.TriangleMesh()
            o3d_mesh.vertices = o3d.utility.Vector3dVector(mesh.vertices)
            o3d_mesh.triangles = o3d.utility.Vector3iVector(mesh.faces)
            
            # Apply smoothing based on method
            if method == 'laplacian':
                o3d_mesh = o3d_mesh.filter_smooth_laplacian(number_of_iterations=iterations)
            elif method == 'taubin':
                o3d_mesh = o3d_mesh.filter_smooth_taubin(number_of_iterations=iterations)
            else:  # simple
                o3d_mesh = o3d_mesh.filter_smooth_simple(number_of_iterations=iterations)
            
            # Ensure we have valid normals
            o3d_mesh.compute_vertex_normals()
            
            # Convert back to trimesh
            vertices = np.asarray(o3d_mesh.vertices)
            faces = np.asarray(o3d_mesh.triangles)
            
            if len(vertices) > 0 and len(faces) > 0:
                smoothed_mesh = trimesh.Trimesh(vertices=vertices, faces=faces)
                logger.info(f"âœ… Mesh smoothing completed")
                return smoothed_mesh
            else:
                logger.warning("âš ï¸ Smoothing failed, returning original mesh")
                return mesh
                
        except Exception as e:
            logger.warning(f"âš ï¸ Mesh smoothing failed: {e}, returning original mesh")
            return mesh
    
    def optimize_mesh_for_smooth_reduction(self, mesh: trimesh.Trimesh, 
                                         target_quality: float = 0.8) -> trimesh.Trimesh:
        """
        Complete mesh optimization pipeline for smooth, hole-free results
        """
        logger.info("ðŸš€ Starting mesh optimization for smooth reduction...")
        
        current_mesh = mesh
        optimization_steps = []
        
        # Step 1: Initial quality assessment
        initial_analysis = self.detect_mesh_holes(current_mesh)
        optimization_steps.append(f"Initial quality: {initial_analysis['quality_score']:.2f}")
        
        logger.info(f"ðŸ“Š Initial mesh analysis:")
        logger.info(f"   Watertight: {initial_analysis['is_watertight']}")
        logger.info(f"   Boundary edges: {initial_analysis['boundary_edges']}")
        logger.info(f"   Quality score: {initial_analysis['quality_score']:.2f}")
        
        # Step 2: Fill holes if needed
        if initial_analysis['has_holes'] or not initial_analysis['is_watertight']:
            if initial_analysis['boundary_edges'] > 100:
                strategy = 'aggressive'
            elif initial_analysis['boundary_edges'] > 20:
                strategy = 'moderate'
            else:
                strategy = 'conservative'
                
            current_mesh = self.fill_mesh_holes(current_mesh, strategy)
            optimization_steps.append(f"Hole filling: {strategy}")
            
            # Re-analyze after hole filling
            post_fill_analysis = self.detect_mesh_holes(current_mesh)
            logger.info(f"ðŸ“ˆ After hole filling: quality {post_fill_analysis['quality_score']:.2f}")
        
        # Step 3: Apply smoothing for perfect surface
        if target_quality > 0.7:
            # High quality target - use conservative smoothing
            smoothing_iterations = 2
            smoothing_method = 'taubin'  # Preserves volume better
        elif target_quality > 0.5:
            # Medium quality - moderate smoothing
            smoothing_iterations = 3
            smoothing_method = 'simple'
        else:
            # Lower quality - more aggressive smoothing
            smoothing_iterations = 5
            smoothing_method = 'laplacian'
        
        current_mesh = self.smooth_mesh(current_mesh, smoothing_method, smoothing_iterations)
        optimization_steps.append(f"Smoothing: {smoothing_method} x{smoothing_iterations}")
        
        # Step 4: Final quality check
        final_analysis = self.detect_mesh_holes(current_mesh)
        optimization_steps.append(f"Final quality: {final_analysis['quality_score']:.2f}")
        
        logger.info(f"ðŸŽ¯ Mesh optimization completed:")
        logger.info(f"   Quality improvement: {initial_analysis['quality_score']:.2f} â†’ {final_analysis['quality_score']:.2f}")
        logger.info(f"   Steps applied: {' â†’ '.join(optimization_steps)}")
        logger.info(f"   Final watertight: {final_analysis['is_watertight']}")
        
        return current_mesh
    
    def adaptive_smooth_reconstruction(self, points: np.ndarray, normals: np.ndarray,
                                     method: str = 'poisson', max_attempts: int = 3) -> Optional[trimesh.Trimesh]:
        """
        Adaptive reconstruction that creates smooth, hole-free meshes
        """
        logger.info("ðŸ”„ Starting adaptive smooth reconstruction...")
        
        # Try different reconstruction parameters to get best smooth result
        reconstruction_configs = [
            {
                'name': 'high_quality_smooth',
                'poisson_depth': 9,
                'poisson_scale': 1.2,
                'alpha_value': 0.015,
                'smoothing_after': ('simple', 2)
            },
            {
                'name': 'medium_quality_smooth', 
                'poisson_depth': 8,
                'poisson_scale': 1.1,
                'alpha_value': 0.020,
                'smoothing_after': ('simple', 3)
            },
            {
                'name': 'robust_smooth',
                'poisson_depth': 7,
                'poisson_scale': 1.0,
                'alpha_value': 0.030,
                'smoothing_after': ('taubin', 5)
            }
        ]
        
        best_mesh = None
        best_quality = 0.0
        
        # Create point cloud
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points)
        pcd.normals = o3d.utility.Vector3dVector(normals)
        
        for i, config in enumerate(reconstruction_configs[:max_attempts]):
            try:
                logger.info(f"ðŸ”„ Attempt {i+1}: {config['name']}")
                
                # Try reconstruction based on method
                if method == 'poisson':
                    mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(
                        pcd, 
                        depth=config['poisson_depth'],
                        width=0,
                        scale=config['poisson_scale'],
                        linear_fit=False
                    )
                elif method == 'alpha_shapes':
                    mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_alpha_shape(
                        pcd, alpha=config['alpha_value'])
                else:
                    # Ball pivoting fallback
                    distances = pcd.compute_nearest_neighbor_distance()
                    avg_dist = np.mean(distances)
                    radius = avg_dist * 2.0
                    mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(
                        pcd, o3d.utility.DoubleVector([radius, radius * 2]))
                
                if len(np.asarray(mesh.vertices)) == 0:
                    continue
                
                # Convert to trimesh
                vertices = np.asarray(mesh.vertices)
                faces = np.asarray(mesh.triangles)
                candidate_mesh = trimesh.Trimesh(vertices=vertices, faces=faces)
                
                # Apply post-reconstruction smoothing
                smooth_method, smooth_iterations = config['smoothing_after']
                candidate_mesh = self.smooth_mesh(candidate_mesh, smooth_method, smooth_iterations)
                
                # Fill any remaining holes
                candidate_mesh = self.fill_mesh_holes(candidate_mesh, 'moderate')
                
                # Evaluate quality
                quality_analysis = self.detect_mesh_holes(candidate_mesh)
                quality_score = quality_analysis['quality_score']
                
                logger.info(f"   Quality score: {quality_score:.2f}")
                logger.info(f"   Watertight: {quality_analysis['is_watertight']}")
                
                # Keep best result
                if quality_score > best_quality:
                    best_quality = quality_score
                    best_mesh = candidate_mesh
                    logger.info(f"   âœ… New best result!")
                
                # If we got a perfect result, stop trying
                if quality_score >= 0.95:
                    logger.info(f"   ðŸŽ¯ Excellent quality achieved, stopping")
                    break
                    
            except Exception as e:
                logger.warning(f"   âš ï¸ Attempt {i+1} failed: {e}")
                continue
        
        if best_mesh is not None:
            logger.info(f"ðŸŽ¯ Adaptive reconstruction completed with quality: {best_quality:.2f}")
            return best_mesh
        else:
            logger.error("âŒ All adaptive reconstruction attempts failed")
            return None


class VertexCalculator:
    """
    NEW: Vertex Calculator for predicting and tracking vertex counts from point cloud reduction
    """
    
    def __init__(self):
        # Vertex estimation models based on reconstruction methods
        self.reconstruction_vertex_ratios = {
            'poisson': {
                'depth_8': 1.2,   # Typically generates 1.2x vertices as input points
                'depth_9': 1.8,   # Higher depth = more vertices
                'depth_10': 2.5,
                'depth_11': 3.2,
                'depth_12': 4.0
            },
            'alpha_shapes': {
                'fine': 0.8,      # Alpha shapes typically reduce vertices
                'medium': 0.6,
                'coarse': 0.4
            },
            'ball_pivoting': {
                'conservative': 0.9,  # Ball pivoting usually creates fewer vertices
                'moderate': 0.7,
                'aggressive': 0.5
            }
        }
        
        # Quality factors for vertex estimation
        self.quality_factors = {
            'surface_roughness_impact': {
                'low': 0.8,      # Smooth surfaces need fewer vertices
                'medium': 1.0,   # Normal factor
                'high': 1.4      # Rough surfaces need more vertices
            },
            'complexity_impact': {
                'low': 0.7,
                'medium': 1.0,
                'high': 1.5
            }
        }
    
    def estimate_vertices_from_points(self, point_count: int, method: str, 
                                    surface_roughness: float, complexity: str) -> Dict:
        """
        Estimate final vertex count from point cloud size and reconstruction parameters
        """
        estimates = {}
        
        # Base estimation by reconstruction method
        if method == 'poisson':
            # Estimate based on typical Poisson behavior
            base_ratio = self.reconstruction_vertex_ratios['poisson']['depth_9']  # Default
            estimates['base_vertices'] = int(point_count * base_ratio)
            
            # Different depth estimations
            for depth, ratio in self.reconstruction_vertex_ratios['poisson'].items():
                estimates[f'vertices_{depth}'] = int(point_count * ratio)
                
        elif method == 'alpha_shapes':
            base_ratio = self.reconstruction_vertex_ratios['alpha_shapes']['medium']
            estimates['base_vertices'] = int(point_count * base_ratio)
            
            # Different alpha parameter estimations
            for level, ratio in self.reconstruction_vertex_ratios['alpha_shapes'].items():
                estimates[f'vertices_{level}'] = int(point_count * ratio)
                
        elif method == 'ball_pivoting':
            base_ratio = self.reconstruction_vertex_ratios['ball_pivoting']['moderate']
            estimates['base_vertices'] = int(point_count * base_ratio)
            
            # Different radius estimations
            for level, ratio in self.reconstruction_vertex_ratios['ball_pivoting'].items():
                estimates[f'vertices_{level}'] = int(point_count * ratio)
        else:
            # Default estimation
            estimates['base_vertices'] = int(point_count * 0.8)
        
        # Apply quality adjustments
        roughness_factor = self._get_roughness_factor(surface_roughness)
        complexity_factor = self.quality_factors['complexity_impact'].get(complexity, 1.0)
        
        adjusted_vertices = int(estimates['base_vertices'] * roughness_factor * complexity_factor)
        estimates['quality_adjusted_vertices'] = adjusted_vertices
        estimates['roughness_factor'] = roughness_factor
        estimates['complexity_factor'] = complexity_factor
        estimates['total_adjustment_factor'] = roughness_factor * complexity_factor
        
        # Range estimation (min/max expected)
        estimates['min_vertices'] = int(adjusted_vertices * 0.7)
        estimates['max_vertices'] = int(adjusted_vertices * 1.4)
        estimates['expected_range'] = f"{estimates['min_vertices']:,} - {estimates['max_vertices']:,}"
        
        return estimates
    
    def _get_roughness_factor(self, surface_roughness: float) -> float:
        """Get adjustment factor based on surface roughness"""
        if surface_roughness < 0.05:
            return self.quality_factors['surface_roughness_impact']['low']
        elif surface_roughness < 0.15:
            return self.quality_factors['surface_roughness_impact']['medium']
        else:
            return self.quality_factors['surface_roughness_impact']['high']
    
    def calculate_reduction_statistics(self, original_vertices: int, original_points: int,
                                     final_points: int, final_vertices: int) -> Dict:
        """
        Calculate comprehensive reduction statistics
        """
        stats = {
            'original_vertices': original_vertices,
            'original_points': original_points,
            'final_points': final_points,
            'final_vertices': final_vertices
        }
        
        # Point reduction ratios
        if original_points > 0:
            stats['point_reduction_ratio'] = final_points / original_points
            stats['point_reduction_percentage'] = (1 - stats['point_reduction_ratio']) * 100
            stats['point_compression_factor'] = original_points / final_points if final_points > 0 else 0
        
        # Vertex reduction ratios
        if original_vertices > 0:
            stats['vertex_reduction_ratio'] = final_vertices / original_vertices
            stats['vertex_reduction_percentage'] = (1 - stats['vertex_reduction_ratio']) * 100
            stats['vertex_compression_factor'] = original_vertices / final_vertices if final_vertices > 0 else 0
        
        # Vertex-to-point ratios
        if original_points > 0:
            stats['original_vertex_to_point_ratio'] = original_vertices / original_points
        if final_points > 0:
            stats['final_vertex_to_point_ratio'] = final_vertices / final_points
        
        # Efficiency metrics
        if final_points > 0 and final_vertices > 0:
            stats['reconstruction_efficiency'] = final_vertices / final_points
            stats['detail_preservation_score'] = min(1.0, stats['reconstruction_efficiency'] * 1.2)
        
        return stats
    
    def predict_memory_usage(self, vertex_count: int, face_count: int = None) -> Dict:
        """
        Predict memory usage for given vertex/face counts
        """
        # Estimate faces if not provided (typical 3D mesh has ~2x faces as vertices)
        if face_count is None:
            face_count = int(vertex_count * 2)
        
        # Memory calculations (approximate)
        vertex_memory_mb = (vertex_count * 3 * 4) / (1024 * 1024)  # 3 floats per vertex
        face_memory_mb = (face_count * 3 * 4) / (1024 * 1024)      # 3 ints per face
        normal_memory_mb = (vertex_count * 3 * 4) / (1024 * 1024)  # 3 floats per normal
        
        total_memory_mb = vertex_memory_mb + face_memory_mb + normal_memory_mb
        
        return {
            'vertices': vertex_count,
            'estimated_faces': face_count,
            'vertex_memory_mb': round(vertex_memory_mb, 2),
            'face_memory_mb': round(face_memory_mb, 2),
            'normal_memory_mb': round(normal_memory_mb, 2),
            'total_memory_mb': round(total_memory_mb, 2),
            'total_memory_gb': round(total_memory_mb / 1024, 3)
        }
    
    def analyze_vertex_efficiency(self, point_count: int, vertex_count: int, 
                                surface_area: float, complexity: str) -> Dict:
        """
        Analyze vertex efficiency and quality metrics
        """
        analysis = {
            'vertex_density': vertex_count / point_count if point_count > 0 else 0,
            'complexity_level': complexity
        }
        
        # Vertex efficiency categories
        if analysis['vertex_density'] > 1.5:
            analysis['efficiency_rating'] = 'high_detail'
            analysis['efficiency_description'] = 'High vertex density - excellent detail preservation'
        elif analysis['vertex_density'] > 1.0:
            analysis['efficiency_rating'] = 'good_detail'
            analysis['efficiency_description'] = 'Good vertex density - balanced detail/performance'
        elif analysis['vertex_density'] > 0.5:
            analysis['efficiency_rating'] = 'moderate_detail'
            analysis['efficiency_description'] = 'Moderate vertex density - performance optimized'
        else:
            analysis['efficiency_rating'] = 'low_detail'
            analysis['efficiency_description'] = 'Low vertex density - highly simplified'
        
        # Surface area considerations
        if surface_area > 0:
            analysis['vertices_per_surface_unit'] = vertex_count / surface_area
            
            if analysis['vertices_per_surface_unit'] > 1000:
                analysis['surface_detail_level'] = 'very_high'
            elif analysis['vertices_per_surface_unit'] > 500:
                analysis['surface_detail_level'] = 'high'
            elif analysis['vertices_per_surface_unit'] > 100:
                analysis['surface_detail_level'] = 'medium'
            else:
                analysis['surface_detail_level'] = 'low'
        
        return analysis


class AdaptiveTargetFinder:
    """
    NEW: Adaptive Target Finder for automatic optimal point detection
    """
    
    def __init__(self):
        # Initialize vertex calculator
        self.vertex_calculator = VertexCalculator()
        
        self.target_strategies = {
            'conservative': {
                'min_multiplier': 2.0,
                'max_multiplier': 5.0,
                'step_multiplier': 1.5,
                'min_absolute': 100,
                'max_absolute': 2000,
                'description': 'Conservative - prioritizes reconstruction success'
            },
            'moderate': {
                'min_multiplier': 1.5,
                'max_multiplier': 3.0,
                'step_multiplier': 1.3,
                'min_absolute': 50,
                'max_absolute': 1500,
                'description': 'Moderate - balanced approach'
            },
            'aggressive': {
                'min_multiplier': 1.2,
                'max_multiplier': 2.5,
                'step_multiplier': 1.2,
                'min_absolute': 30,
                'max_absolute': 1000,
                'description': 'Aggressive - stays closer to user target'
            }
        }
        
        # Quality thresholds for target validation
        self.quality_thresholds = {
            'min_points_ratio': 0.001,      # At least 0.1% of original
            'max_points_ratio': 0.8,        # At most 80% of original
            'min_reconstruction_points': 20, # Absolute minimum for reconstruction
            'max_reconstruction_attempts': 5 # Maximum retry attempts per strategy
        }
    
    def analyze_model_complexity(self, points: np.ndarray, analysis: Dict) -> str:
        """Analyze model complexity to choose appropriate target strategy"""
        n_points = len(points)
        surface_roughness = analysis.get('surface_roughness', 0.05)
        complexity = analysis.get('complexity', 'medium')
        requires_detail = analysis.get('requires_high_detail', False)
        
        # Determine complexity level for target strategy selection
        if (complexity == 'high' or surface_roughness > 0.2 or 
            requires_detail or n_points > 100000):
            return 'conservative'
        elif complexity == 'low' and surface_roughness < 0.05 and n_points < 20000:
            return 'aggressive'
        else:
            return 'moderate'
    
    def generate_target_candidates(self, original_count: int, user_target: int, 
                                 strategy: str, analysis: Dict) -> List[int]:
        """Generate list of target candidates based on strategy"""
        strategy_config = self.target_strategies[strategy]
        
        # Calculate target range
        min_target = max(
            int(user_target * strategy_config['min_multiplier']),
            strategy_config['min_absolute'],
            self.quality_thresholds['min_reconstruction_points']
        )
        
        max_target = min(
            int(user_target * strategy_config['max_multiplier']),
            strategy_config['max_absolute'],
            int(original_count * self.quality_thresholds['max_points_ratio'])
        )
        
        # Ensure valid range
        if min_target >= max_target:
            max_target = min_target + 50
        
        # Generate candidates with progressive scaling
        candidates = []
        current = min_target
        multiplier = strategy_config['step_multiplier']
        
        while current <= max_target and len(candidates) < 10:  # Limit candidates
            candidates.append(int(current))
            current *= multiplier
        
        # Always include the max target
        if max_target not in candidates:
            candidates.append(max_target)
        
        # Sort candidates (smallest first for aggressive reduction testing)
        candidates.sort()
        
        logger.info(f"ðŸŽ¯ Generated {len(candidates)} target candidates for {strategy} strategy")
        logger.info(f"   Range: {min(candidates)} to {max(candidates)} points")
        logger.info(f"   User target: {user_target}, Original: {original_count}")
        
        return candidates
    
    def validate_target_feasibility(self, target: int, original_count: int, 
                                   analysis: Dict) -> Tuple[bool, str]:
        """Validate if a target is feasible for the given model"""
        ratio = target / original_count
        
        # Check minimum ratio
        if ratio < self.quality_thresholds['min_points_ratio']:
            return False, f"Target too small: {ratio:.4f} < {self.quality_thresholds['min_points_ratio']}"
        
        # Check maximum ratio
        if ratio > self.quality_thresholds['max_points_ratio']:
            return False, f"Target too large: {ratio:.4f} > {self.quality_thresholds['max_points_ratio']}"
        
        # Check absolute minimum
        if target < self.quality_thresholds['min_reconstruction_points']:
            return False, f"Below minimum reconstruction points: {target} < {self.quality_thresholds['min_reconstruction_points']}"
        
        # Check complexity-based constraints
        complexity = analysis.get('complexity', 'medium')
        surface_roughness = analysis.get('surface_roughness', 0.05)
        
        if complexity == 'high' and ratio < 0.01:  # High complexity needs more points
            return False, f"High complexity model needs more points: {ratio:.4f} < 0.01"
        
        if surface_roughness > 0.3 and ratio < 0.02:  # Very rough surfaces need more points
            return False, f"Very rough surface needs more points: roughness={surface_roughness:.3f}, ratio={ratio:.4f}"
        
        return True, "Target feasible"
    
    def find_optimal_target(self, original_count: int, user_target: int, 
                           analysis: Dict, adaptive_enabled: bool = True) -> Dict:
        """
        Find the optimal target point count for successful reduction and reconstruction
        """
        logger.info("ðŸ” Starting adaptive target finding...")
        
        if not adaptive_enabled:
            logger.info("ðŸ”’ Adaptive targeting disabled, using user target")
            return {
                'target': user_target,
                'strategy': 'user_specified',
                'candidates_tested': 1,
                'feasible': True,
                'reason': 'User specified, adaptive disabled'
            }
        
        # Determine best strategy based on model complexity
        recommended_strategy = self.analyze_model_complexity(original_count, analysis)
        logger.info(f"ðŸ“Š Recommended strategy: {recommended_strategy}")
        logger.info(f"   {self.target_strategies[recommended_strategy]['description']}")
        
        # Try strategies in order of preference
        strategy_order = [recommended_strategy]
        
        # Add fallback strategies
        for strategy in ['conservative', 'moderate', 'aggressive']:
            if strategy not in strategy_order:
                strategy_order.append(strategy)
        
        best_result = None
        
        for strategy in strategy_order:
            logger.info(f"ðŸ§ª Testing {strategy} strategy...")
            
            # Generate candidates for this strategy
            candidates = self.generate_target_candidates(
                original_count, user_target, strategy, analysis)
            
            # Test each candidate
            for i, target in enumerate(candidates):
                feasible, reason = self.validate_target_feasibility(
                    target, original_count, analysis)
                
                if feasible:
                    result = {
                        'target': target,
                        'strategy': strategy,
                        'candidates_tested': i + 1,
                        'feasible': True,
                        'reason': f'Found feasible target with {strategy} strategy',
                        'ratio': target / original_count,
                        'user_ratio': user_target / original_count,
                        'adjustment_factor': target / user_target
                    }
                    
                    logger.info(f"âœ… Found feasible target: {target} points")
                    logger.info(f"   Strategy: {strategy}, Ratio: {result['ratio']:.4f}")
                    logger.info(f"   Adjustment: {result['adjustment_factor']:.1f}x user target")
                    
                    # Save the first feasible result as best
                    if best_result is None:
                        best_result = result
                    
                    # For aggressive strategy, take first feasible result
                    if strategy == 'aggressive':
                        return result
                    
                    # For moderate/conservative, continue to find better options
                    break
                else:
                    logger.debug(f"âŒ Target {target} not feasible: {reason}")
        
        # Return best result found, or create emergency fallback
        if best_result is not None:
            return best_result
        
        # Emergency fallback - use a safe conservative target
        emergency_target = max(
            self.quality_thresholds['min_reconstruction_points'],
            int(original_count * 0.05),  # 5% of original
            user_target * 3  # 3x user target
        )
        
        # Cap emergency target
        emergency_target = min(emergency_target, int(original_count * 0.3))
        
        logger.warning(f"âš ï¸ Using emergency fallback target: {emergency_target}")
        
        return {
            'target': emergency_target,
            'strategy': 'emergency_fallback',
            'candidates_tested': 0,
            'feasible': True,
            'reason': 'Emergency fallback - all strategies failed',
            'ratio': emergency_target / original_count,
            'user_ratio': user_target / original_count,
            'adjustment_factor': emergency_target / user_target
        }


class BallastQualitySpecialist:
    """
    ENHANCED ballast specialist with superior detail preservation
    """
    
    def __init__(self):
        # Enhanced config for much better detail preservation
        self.ballast_config = {
            # Multi-scale analysis
            'fine_scale_neighbors': 8,
            'medium_scale_neighbors': 15,
            'coarse_scale_neighbors': 25,
            
            # Enhanced clustering (much more conservative)
            'epsilon_detail': 0.002,        # Very fine for high detail areas
            'epsilon_fine': 0.005,          # Fine for medium detail areas
            'epsilon_medium': 0.010,        # Medium detail areas
            'epsilon_coarse': 0.020,        # Lower detail areas
            
            # Surface complexity thresholds
            'high_complexity_threshold': 0.2,   # Very complex surfaces
            'medium_complexity_threshold': 0.1, # Medium complexity
            'low_complexity_threshold': 0.05,   # Smoother areas
            
            # Enhanced importance preservation (much more conservative)
            'importance_threshold_aggressive': 25,  # Top 75% of features
            'importance_threshold_moderate': 35,    # Top 65% of features
            'importance_threshold_conservative': 50, # Top 50% of features
            
            # Surface reconstruction settings
            'alpha_shape_alpha_fine': 0.006,     # Very fine alpha shapes
            'alpha_shape_alpha_coarse': 0.015,   # Coarser fallback
            'ball_pivot_radius_factor': 0.6,     # Conservative ball pivoting
            'poisson_depth_ultra': 12,           # Ultra high detail
            'poisson_depth_high': 11,            # High detail
            'poisson_depth_medium': 9,           # Medium detail
            'poisson_depth_low': 8,              # Lower detail
            
            # Quality validation
            'min_points_for_reconstruction': 20, # Lower minimum
            'max_reconstruction_attempts': 6,    # More attempts
            'normal_estimation_neighbors': 15,   # Fewer neighbors for speed
        }
        
        # Initialize adaptive target finder, vertex calculator, and mesh smoother
        self.adaptive_finder = AdaptiveTargetFinder()
        self.vertex_calculator = VertexCalculator()
        self.mesh_smoother = MeshSmoothingAndRepair()  # NEW
        
    def detect_ballast_model(self, file_path: str) -> bool:
        """Enhanced ballast detection"""
        filename = file_path.lower()
        ballast_keywords = ['ballast', 'stone', 'rock', 'aggregate', 'gravel', 'bpk', 'rail', 'track']
        return any(keyword in filename for keyword in ballast_keywords)
    
    def multi_scale_surface_analysis(self, points: np.ndarray) -> Dict:
        """Enhanced multi-scale surface analysis for better detail detection"""
        n_points = len(points)
        analysis = {'complexity_zones': {}, 'detail_map': np.zeros(n_points)}
        
        if n_points < 20:
            return analysis
        
        # Multi-scale neighbor analysis
        for scale, k_neighbors in [
            ('fine', self.ballast_config['fine_scale_neighbors']),
            ('medium', self.ballast_config['medium_scale_neighbors']),
            ('coarse', self.ballast_config['coarse_scale_neighbors'])
        ]:
            k = min(k_neighbors, n_points - 1)
            if k < 3:
                continue
                
            nbrs = NearestNeighbors(n_neighbors=k)
            nbrs.fit(points)
            distances, indices = nbrs.kneighbors(points)
            
            # Calculate surface variation at this scale
            surface_variation = np.std(distances[:, 1:], axis=1)
            local_density = np.mean(distances[:, 1:], axis=1)
            
            # Combine into complexity measure
            complexity = surface_variation / (local_density + 1e-8)
            weight = 1.0 if scale == 'fine' else 0.7 if scale == 'medium' else 0.5
            analysis['detail_map'] += complexity * weight
        
        # Normalize detail map
        if np.max(analysis['detail_map']) > 0:
            analysis['detail_map'] /= np.max(analysis['detail_map'])
        
        # Identify complexity zones
        high_detail = analysis['detail_map'] > self.ballast_config['high_complexity_threshold']
        medium_detail = (analysis['detail_map'] > self.ballast_config['medium_complexity_threshold']) & ~high_detail
        low_detail = analysis['detail_map'] <= self.ballast_config['medium_complexity_threshold']
        
        analysis['complexity_zones'] = {
            'high_detail': np.where(high_detail)[0],
            'medium_detail': np.where(medium_detail)[0], 
            'low_detail': np.where(low_detail)[0]
        }
        
        logger.info(f"ðŸ” Multi-scale analysis: {np.sum(high_detail)} high, {np.sum(medium_detail)} medium, {np.sum(low_detail)} low detail points")
        
        return analysis

    def analyze_ballast_complexity(self, points: np.ndarray) -> Dict:
        """Enhanced ballast complexity analysis with multi-scale surface understanding"""
        n_points = len(points)
        
        # Calculate geometric complexity
        bbox = np.max(points, axis=0) - np.min(points, axis=0)
        bbox_volume = np.prod(bbox)
        bbox_surface_area = 2 * (bbox[0]*bbox[1] + bbox[1]*bbox[2] + bbox[0]*bbox[2])
        
        # Enhanced surface roughness analysis
        if n_points > 100:
            # Sample points to estimate surface variation
            sample_size = min(1000, n_points)
            sample_indices = np.random.choice(n_points, sample_size, replace=False)
            sample_points = points[sample_indices]
            
            # Compute distances to nearest neighbors
            if sample_size > 10:
                nbrs = NearestNeighbors(n_neighbors=min(10, sample_size-1))
                nbrs.fit(sample_points)
                distances, _ = nbrs.kneighbors(sample_points)
                avg_neighbor_distance = np.mean(distances[:, 1:])  # Exclude self
                surface_roughness = np.std(distances[:, 1:])
            else:
                avg_neighbor_distance = 0.1
                surface_roughness = 0.05
        else:
            avg_neighbor_distance = 0.1
            surface_roughness = 0.05
        
        # Classify ballast complexity
        if bbox_volume > 1000 or n_points > 100000:
            complexity = "high"
        elif bbox_volume > 100 or n_points > 20000:
            complexity = "medium"
        else:
            complexity = "low"
        
        # Perform multi-scale surface analysis
        surface_analysis = self.multi_scale_surface_analysis(points)
        
        analysis = {
            'complexity': complexity,
            'bbox_volume': bbox_volume,
            'bbox_surface_area': bbox_surface_area,
            'surface_roughness': surface_roughness,
            'avg_neighbor_distance': avg_neighbor_distance,
            'original_points': n_points,
            'surface_analysis': surface_analysis,
            'requires_high_detail': surface_roughness > 0.15 or np.sum(surface_analysis['detail_map'] > 0.2) > n_points * 0.1
        }
        
        logger.info(f"ðŸ” Enhanced ballast analysis: {complexity} complexity, roughness: {surface_roughness:.4f}")
        
        return analysis
    
    def predict_vertex_outcomes(self, point_count: int, method: str, analysis: Dict) -> Dict:
        """
        Predict vertex counts and quality metrics before processing
        """
        surface_roughness = analysis.get('surface_roughness', 0.05)
        complexity = analysis.get('complexity', 'medium')
        surface_area = analysis.get('bbox_surface_area', 1000)
        
        # Get vertex estimations
        vertex_estimates = self.vertex_calculator.estimate_vertices_from_points(
            point_count, method, surface_roughness, complexity)
        
        # Predict memory usage
        memory_prediction = self.vertex_calculator.predict_memory_usage(
            vertex_estimates['quality_adjusted_vertices'])
        
        # Analyze expected efficiency
        efficiency_analysis = self.vertex_calculator.analyze_vertex_efficiency(
            point_count, vertex_estimates['quality_adjusted_vertices'], 
            surface_area, complexity)
        
        prediction = {
            'method': method,
            'input_points': point_count,
            'vertex_estimates': vertex_estimates,
            'memory_prediction': memory_prediction,
            'efficiency_analysis': efficiency_analysis,
            'quality_summary': {
                'expected_detail_level': efficiency_analysis.get('efficiency_rating', 'unknown'),
                'description': efficiency_analysis.get('efficiency_description', 'Unknown quality level'),
                'vertex_density': efficiency_analysis.get('vertex_density', 0),
                'reconstruction_factor': vertex_estimates.get('total_adjustment_factor', 1.0)
            }
        }
        
        logger.info(f"ðŸ“Š VERTEX PREDICTION for {method} reconstruction:")
        logger.info(f"   Input points: {point_count:,}")
        logger.info(f"   Expected vertices: {vertex_estimates['quality_adjusted_vertices']:,}")
        logger.info(f"   Range: {vertex_estimates['expected_range']}")
        logger.info(f"   Quality level: {prediction['quality_summary']['expected_detail_level']}")
        logger.info(f"   Memory usage: {memory_prediction['total_memory_mb']:.1f} MB")
        
        return prediction
    
    def calculate_final_vertex_statistics(self, original_vertices: int, original_points: int,
                                        final_points: int, final_mesh) -> Dict:
        """
        Calculate comprehensive vertex reduction statistics after processing
        """
        final_vertices = len(final_mesh.vertices) if final_mesh else 0
        
        # Calculate reduction statistics
        stats = self.vertex_calculator.calculate_reduction_statistics(
            original_vertices, original_points, final_points, final_vertices)
        
        # Additional mesh statistics
        if final_mesh:
            stats['final_faces'] = len(final_mesh.faces)
            stats['vertex_to_face_ratio'] = final_vertices / len(final_mesh.faces) if len(final_mesh.faces) > 0 else 0
            
            # Mesh quality metrics
            try:
                stats['mesh_volume'] = final_mesh.volume
                stats['mesh_surface_area'] = final_mesh.area
                stats['mesh_watertight'] = final_mesh.is_watertight
            except:
                stats['mesh_volume'] = 0
                stats['mesh_surface_area'] = 0
                stats['mesh_watertight'] = False
        
        # Memory usage comparison
        original_memory = self.vertex_calculator.predict_memory_usage(original_vertices)
        final_memory = self.vertex_calculator.predict_memory_usage(final_vertices)
        
        stats['memory_reduction'] = {
            'original_mb': original_memory['total_memory_mb'],
            'final_mb': final_memory['total_memory_mb'],
            'reduction_mb': original_memory['total_memory_mb'] - final_memory['total_memory_mb'],
            'reduction_percentage': ((original_memory['total_memory_mb'] - final_memory['total_memory_mb']) / 
                                   original_memory['total_memory_mb'] * 100) if original_memory['total_memory_mb'] > 0 else 0
        }
        
        logger.info(f"ðŸ“ˆ FINAL VERTEX STATISTICS:")
        logger.info(f"   Vertices: {original_vertices:,} â†’ {final_vertices:,} ({stats.get('vertex_reduction_percentage', 0):.1f}% reduction)")
        logger.info(f"   Points: {original_points:,} â†’ {final_points:,} ({stats.get('point_reduction_percentage', 0):.1f}% reduction)")
        if final_mesh:
            logger.info(f"   Faces: {len(final_mesh.faces):,}")
            logger.info(f"   Vertex density: {stats.get('final_vertex_to_point_ratio', 0):.2f}")
        logger.info(f"   Memory: {original_memory['total_memory_mb']:.1f} MB â†’ {final_memory['total_memory_mb']:.1f} MB")
        logger.info(f"   Memory reduction: {stats['memory_reduction']['reduction_percentage']:.1f}%")
        
        return stats

    def find_adaptive_target_points(self, original_points: np.ndarray, 
                                   user_target: int, analysis: Dict, 
                                   adaptive_enabled: bool = True) -> Dict:
        """
        NEW: Use adaptive target finder to get optimal target points
        """
        return self.adaptive_finder.find_optimal_target(
            len(original_points), user_target, analysis, adaptive_enabled)
    
    def get_quality_focused_target_points(self, original_points: np.ndarray, 
                                        target_ratio: float, analysis: Dict) -> int:
        """
        Calculate target points that RESPECTS user target while ensuring minimum quality
        """
        original_count = len(original_points)
        base_target = int(original_count * target_ratio)
        
        # Much more reasonable quality adjustments that still respect user intent
        if base_target < 50:  # Very small targets
            # For very small targets, be more generous but reasonable
            quality_multiplier = 3.0  # 3x more points max
        elif base_target < 200:  # Small targets  
            quality_multiplier = 2.0  # 2x more points max
        elif base_target < 1000:  # Medium targets
            quality_multiplier = 1.5  # 1.5x more points max
        else:  # Large targets
            quality_multiplier = 1.2  # Only 20% more for large targets
        
        # Apply complexity-based adjustment (much more modest)
        if analysis['complexity'] == 'high' and analysis['surface_roughness'] > 0.3:
            quality_multiplier *= 1.3  # Extra 30% for very complex surfaces
        elif analysis['surface_roughness'] > 0.1:
            quality_multiplier *= 1.1  # Extra 10% for rough surfaces
        
        # Calculate quality-adjusted target
        adjusted_target = int(base_target * quality_multiplier)
        
        # Ensure minimum viable points for reconstruction
        min_viable = 30  # Absolute minimum
        optimal_points = max(adjusted_target, min_viable)
        
        # Cap at reasonable maximum relative to user intent
        max_allowed = min(base_target * 5, 2000)  # Never more than 5x user target or 2000 points
        optimal_points = min(optimal_points, max_allowed)
        
        logger.info(f"ðŸŽ¯ Balanced target: {original_count:,} â†’ {optimal_points:,} points")
        logger.info(f"   User target: {base_target:,}, Quality-adjusted: {optimal_points:,}")
        logger.info(f"   Quality multiplier: {quality_multiplier:.1f}x (respects user intent)")
        
        return optimal_points
    
    def get_ballast_quality_parameters(self, analysis: Dict, target_points: int, original_points: int, aggressive_reduction: bool = False) -> Dict:
        """
        Get parameters optimized for ballast quality preservation while respecting targets
        """
        complexity = analysis['complexity']
        surface_roughness = analysis['surface_roughness']
        reduction_ratio = target_points / original_points
        
        # Adjust parameters based on how aggressive the reduction needs to be
        if reduction_ratio < 0.02:  # Very aggressive reduction (< 2%)
            importance_threshold = self.ballast_config['importance_threshold_aggressive']
            epsilon_scale = 1.5  # Larger epsilon for more aggressive clustering
            k_neighbors = 8
        elif reduction_ratio < 0.05:  # Moderate aggressive reduction (< 5%)
            importance_threshold = self.ballast_config['importance_threshold_moderate'] 
            epsilon_scale = 1.2
            k_neighbors = 6
        else:  # Conservative reduction (>= 5%)
            importance_threshold = self.ballast_config['importance_threshold_conservative']
            epsilon_scale = 1.0
            k_neighbors = 6
        
        # Choose base clustering parameters based on surface detail
        if surface_roughness > 0.1:  # Very detailed surface
            base_epsilon = self.ballast_config['epsilon_fine']
        elif surface_roughness > 0.05:  # Moderately detailed
            base_epsilon = self.ballast_config['epsilon_medium']
        else:  # Smoother surface
            base_epsilon = self.ballast_config['epsilon_coarse']
        
        # Apply scaling based on reduction aggressiveness
        epsilon = base_epsilon * epsilon_scale
        
        # DBSCAN cleanup parameters (scale with aggressiveness)
        dbscan_eps = epsilon * (1.5 if reduction_ratio > 0.05 else 2.0)
        
        params = {
            'k_neighbors': k_neighbors,
            'epsilon': epsilon,
            'dbscan_eps': dbscan_eps,
            'importance_threshold': importance_threshold,
            'complexity': complexity,
            'surface_roughness': surface_roughness,
            'reduction_ratio': reduction_ratio,
            'epsilon_scale': epsilon_scale
        }
        
        logger.info(f"ðŸŽ›ï¸ Target-aware parameters: {params}")
        
        return params
    
    def enhanced_feature_extraction_for_ballast(self, points: np.ndarray, surface_analysis: Dict, k_neighbors: int = 12) -> np.ndarray:
        """
        Enhanced feature extraction with multi-scale surface understanding and 8 features
        """
        n_points = len(points)
        features = np.zeros((n_points, 8), dtype=np.float32)  # Enhanced to 8 features
        
        if n_points < 4:
            return features
        
        # Use adaptive neighbors based on local complexity
        detail_map = surface_analysis.get('detail_map', np.ones(n_points))
        
        # Adaptive neighbor selection based on surface complexity
        base_neighbors = k_neighbors
        max_neighbors = min(25, n_points - 1)
        
        # Global features first
        centroid = np.mean(points, axis=0)
        centroid_distances = np.linalg.norm(points - centroid, axis=1)
        
        for i in range(n_points):
            # More neighbors for complex areas
            complexity = detail_map[i]
            k = min(max_neighbors, int(base_neighbors * (1 + complexity * 0.5)))
            
            # Find neighbors
            distances = np.linalg.norm(points - points[i], axis=1)
            neighbor_indices = np.argsort(distances)[1:k+1]  # Exclude self
            neighbor_points = points[neighbor_indices]
            neighbor_distances = distances[neighbor_indices]
            
            if len(neighbor_points) < 3:
                continue
                
            # Feature 1: Local density (mean distance)
            features[i, 0] = np.mean(neighbor_distances)
            
            # Feature 2: Surface variation (std of distances) - crucial for ballast
            features[i, 1] = np.std(neighbor_distances)
            
            # Feature 3: Maximum neighbor distance (edge detection)
            features[i, 2] = np.max(neighbor_distances)
            
            # Feature 4 & 5: Enhanced curvature estimation (primary and secondary)
            try:
                centered = neighbor_points - np.mean(neighbor_points, axis=0)
                cov_matrix = np.cov(centered.T)
                eigenvals = np.linalg.eigvals(cov_matrix)
                eigenvals = np.sort(eigenvals)[::-1]
                
                if eigenvals[0] > 1e-10:
                    features[i, 3] = eigenvals[2] / eigenvals[0]  # Primary curvature
                    features[i, 4] = eigenvals[1] / eigenvals[0]  # Secondary curvature
                else:
                    features[i, 3] = 0
                    features[i, 4] = 0
            except:
                features[i, 3] = 0
                features[i, 4] = 0
            
            # Feature 6: Surface roughness ratio
            features[i, 5] = features[i, 1] / (features[i, 0] + 1e-8)
            
            # Feature 7: Local planarity measure
            if len(neighbor_points) >= 3:
                try:
                    # Fit plane to neighbors
                    A = np.column_stack([neighbor_points[:, :2], np.ones(len(neighbor_points))])
                    z = neighbor_points[:, 2]
                    plane_params, residuals, _, _ = np.linalg.lstsq(A, z, rcond=None)
                    features[i, 6] = np.sqrt(np.mean(residuals)) if len(residuals) > 0 else 0
                except:
                    features[i, 6] = 0
            
            # Feature 8: Multi-scale complexity (from analysis)
            features[i, 7] = complexity
        
        # Normalize centroid distances and add as weight to feature 0
        if np.max(centroid_distances) > 0:
            centroid_distances = centroid_distances / np.max(centroid_distances)
        features[:, 0] += centroid_distances * 0.1  # Small contribution
        
        logger.debug(f"âœ… Enhanced 8-feature extraction for {n_points:,} points")
        return features

    def create_ballast_importance_labels(self, features: np.ndarray, points: np.ndarray,
                                       surface_analysis: Dict, importance_threshold: int = 50) -> np.ndarray:
        """
        Enhanced multi-pass importance detection with zone-specific processing
        """
        n_points = len(points)
        importance_scores = np.zeros(n_points)
        
        zones = surface_analysis.get('complexity_zones', {})
        detail_map = surface_analysis.get('detail_map', np.ones(n_points))
        
        # Pass 1: Basic geometric importance using enhanced 8 features
        geometric_importance = (
            features[:, 1] * 3.0 +    # Surface variation (most important for ballast)
            features[:, 2] * 2.5 +    # Max distance (edge detection)
            features[:, 3] * 2.0 +    # Primary curvature
            features[:, 4] * 1.5 +    # Secondary curvature
            features[:, 5] * 2.0 +    # Surface roughness
            features[:, 6] * 1.0 +    # Planarity deviation
            features[:, 7] * 1.0      # Multi-scale complexity
        )
        
        # Pass 2: Multi-scale complexity weighting
        complexity_weight = 1.0 + detail_map * 2.0  # Boost complex areas
        scaled_importance = geometric_importance * complexity_weight
        
        # Pass 3: Zone-specific adjustments
        for zone_name, zone_indices in zones.items():
            if len(zone_indices) == 0:
                continue
                
            if zone_name == 'high_detail':
                # Much higher importance for high detail areas
                scaled_importance[zone_indices] *= 1.5
            elif zone_name == 'medium_detail':
                # Moderate boost for medium detail
                scaled_importance[zone_indices] *= 1.2
            # Low detail areas keep base importance
        
        # Pass 4: Enhanced edge and corner detection
        boundary_boost = self._detect_boundary_points_enhanced(points, features)
        scaled_importance += boundary_boost
        
        # Normalize final scores
        if np.max(scaled_importance) > 0:
            importance_scores = scaled_importance / np.max(scaled_importance)
        
        # Apply threshold
        threshold = np.percentile(importance_scores, importance_threshold)
        pseudo_labels = (importance_scores >= threshold).astype(int)
        
        logger.info(f"ðŸŽ¯ Enhanced multi-pass importance: {np.sum(pseudo_labels):,}/{n_points:,} important points")
        logger.info(f"   Importance threshold: {importance_threshold}% (keeping top {100-importance_threshold}%)")
        
        return pseudo_labels
    
    def _detect_boundary_points_enhanced(self, points: np.ndarray, features: np.ndarray) -> np.ndarray:
        """Enhanced boundary and edge point detection"""
        n_points = len(points)
        boundary_scores = np.zeros(n_points)
        
        if n_points < 10:
            return boundary_scores
        
        # Method 1: Low local density indicates boundary
        local_density = features[:, 0]
        mean_density = np.mean(local_density)
        boundary_scores += (mean_density - local_density) / (mean_density + 1e-8)
        
        # Method 2: High surface variation indicates edges
        surface_variation = features[:, 1]
        if np.max(surface_variation) > 0:
            boundary_scores += surface_variation / np.max(surface_variation)
        
        # Method 3: Convex hull points are definitely boundary (if scipy available)
        if SCIPY_AVAILABLE:
            try:
                if n_points >= 4:
                    hull = ConvexHull(points)
                    boundary_scores[hull.vertices] += 1.0
            except:
                pass
        
        # Method 4: High curvature points
        curvature = features[:, 3] + features[:, 4]
        if np.max(curvature) > 0:
            boundary_scores += curvature / np.max(curvature) * 0.5
        
        # Normalize
        if np.max(boundary_scores) > 0:
            boundary_scores = boundary_scores / np.max(boundary_scores) * 0.5
        
        return boundary_scores

    def quality_focused_surface_reconstruction(self, points: np.ndarray, normals: np.ndarray, 
                                             method: str = 'poisson', 
                                             enable_smoothing: bool = True) -> Optional[trimesh.Trimesh]:
        """
        Enhanced texture-preserving surface reconstruction with hole filling and smoothing
        """
        if len(points) < self.ballast_config['min_points_for_reconstruction']:
            logger.warning(f"âš ï¸ Too few points ({len(points)}) for quality reconstruction")
            return None
        
        logger.info(f"ðŸ”§ Enhanced reconstruction for {len(points):,} points (smoothing: {'ON' if enable_smoothing else 'OFF'})")
        
        # Enhance normals first
        try:
            normals = self.improve_normals_for_ballast(points, normals)
        except Exception as e:
            logger.warning(f"âš ï¸ Normal enhancement failed: {e}")
        
        # Try adaptive smooth reconstruction first (NEW)
        if enable_smoothing:
            logger.info("ðŸŽ¨ Attempting adaptive smooth reconstruction...")
            smooth_mesh = self.mesh_smoother.adaptive_smooth_reconstruction(
                points, normals, method, max_attempts=3)
            
            if smooth_mesh is not None:
                # Validate the smooth result
                hole_analysis = self.mesh_smoother.detect_mesh_holes(smooth_mesh)
                if hole_analysis['quality_score'] >= 0.7:
                    logger.info(f"âœ… Smooth reconstruction successful! Quality: {hole_analysis['quality_score']:.2f}")
                    logger.info(f"   Watertight: {hole_analysis['is_watertight']}")
                    logger.info(f"   Vertices: {len(smooth_mesh.vertices):,}, Faces: {len(smooth_mesh.faces):,}")
                    return smooth_mesh
                else:
                    logger.info(f"âš ï¸ Smooth reconstruction quality too low ({hole_analysis['quality_score']:.2f}), trying standard methods...")
        
        # Fallback to enhanced standard reconstruction with post-processing
        logger.info("ðŸ”„ Using enhanced standard reconstruction...")
        
        # Create point cloud
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points)
        pcd.normals = o3d.utility.Vector3dVector(normals)
        
        # Enhanced reconstruction attempts (ordered by detail preservation)
        reconstruction_attempts = [
            ('poisson_optimized', self._try_poisson_for_smoothness),
            ('alpha_shapes_fine', self._try_alpha_shapes_fine),
            ('ball_pivoting_enhanced', self._try_ball_pivoting_enhanced),
            ('poisson_ultra_high', self._try_poisson_ultra_high),
            ('alpha_shapes_coarse', self._try_alpha_shapes_coarse),
            ('poisson_high', self._try_poisson_high_quality),
            ('poisson_medium', self._try_poisson_medium_quality),
            ('poisson_low', self._try_poisson_low_quality)
        ]
        
        for attempt_name, reconstruction_func in reconstruction_attempts:
            try:
                logger.info(f"ðŸ”„ Trying {attempt_name} reconstruction...")
                mesh = reconstruction_func(pcd)
                
                if mesh is not None:
                    vertices = np.asarray(mesh.vertices)
                    faces = np.asarray(mesh.triangles)
                    
                    if len(faces) > 0:
                        reconstructed_mesh = trimesh.Trimesh(vertices=vertices, faces=faces)
                        
                        # NEW: Apply post-reconstruction optimization for smoothness
                        if enable_smoothing:
                            logger.info("ðŸŽ¨ Applying post-reconstruction smoothing and hole filling...")
                            reconstructed_mesh = self.mesh_smoother.optimize_mesh_for_smooth_reduction(
                                reconstructed_mesh, target_quality=0.7)
                        
                        # Enhanced validation for texture preservation AND smoothness
                        if self._validate_smooth_mesh_quality(reconstructed_mesh, points, enable_smoothing):
                            logger.info(f"âœ… Success with {attempt_name}: {len(vertices):,} vertices, {len(faces):,} faces")
                            
                            # Final quality report
                            if enable_smoothing:
                                final_analysis = self.mesh_smoother.detect_mesh_holes(reconstructed_mesh)
                                logger.info(f"ðŸ“Š Final mesh quality: {final_analysis['quality_score']:.2f}")
                                logger.info(f"   Watertight: {final_analysis['is_watertight']}")
                                logger.info(f"   Boundary edges: {final_analysis['boundary_edges']}")
                            
                            return reconstructed_mesh
                        else:
                            logger.warning(f"âš ï¸ {attempt_name} failed quality validation")
                    else:
                        logger.warning(f"âš ï¸ {attempt_name} produced no faces")
                else:
                    logger.warning(f"âš ï¸ {attempt_name} returned None")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ {attempt_name} failed: {e}")
                continue
        
        logger.error(f"âŒ All enhanced reconstruction methods failed")
        return None
    
    def _try_poisson_for_smoothness(self, pcd) -> Optional[o3d.geometry.TriangleMesh]:
        """NEW: Poisson reconstruction optimized for smoothness and hole-free results"""
        mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(
            pcd, 
            depth=8,      # Balanced depth for smoothness
            width=0,
            scale=1.15,   # Slightly larger scale for better coverage
            linear_fit=False
        )
        
        # Immediate light smoothing
        if len(np.asarray(mesh.vertices)) > 0:
            mesh = mesh.filter_smooth_simple(number_of_iterations=1)
            mesh.compute_vertex_normals()
        
        return mesh
    
    def _validate_smooth_mesh_quality(self, mesh: trimesh.Trimesh, original_points: np.ndarray, 
                                    check_smoothness: bool = True) -> bool:
        """
        Enhanced validation focusing on both texture preservation AND smoothness
        """
        try:
            # Basic checks
            if len(mesh.vertices) < 4 or len(mesh.faces) < 4:
                return False
            
            # Check for reasonable simplification (allow more vertices for quality)
            vertex_ratio = len(mesh.vertices) / len(original_points)
            if vertex_ratio < 0.02:  # Less than 2% is too aggressive
                logger.warning(f"âš ï¸ Mesh too simplified: {vertex_ratio:.3f} vertex ratio")
                return False
            
            # Check bounding box preservation
            original_bbox = np.max(original_points, axis=0) - np.min(original_points, axis=0)
            mesh_bbox = mesh.bounds[1] - mesh.bounds[0]
            
            bbox_ratio = np.linalg.norm(mesh_bbox) / np.linalg.norm(original_bbox)
            if bbox_ratio < 0.5 or bbox_ratio > 3.0:  # More lenient for smooth meshes
                logger.warning(f"âš ï¸ Mesh size changed too much: {bbox_ratio:.2f}")
                return False
            
            # NEW: Check for smoothness and holes if enabled
            if check_smoothness:
                hole_analysis = self.mesh_smoother.detect_mesh_holes(mesh)
                
                # Accept if quality is reasonable (not necessarily perfect)
                if hole_analysis['quality_score'] < 0.5:  # Below 50% quality
                    logger.warning(f"âš ï¸ Mesh quality too low: {hole_analysis['quality_score']:.2f}")
                    return False
                
                # Check for excessive boundary edges (indicates holes)
                if hole_analysis['boundary_edges'] > len(mesh.vertices) * 0.1:  # More than 10% boundary
                    logger.warning(f"âš ï¸ Too many holes: {hole_analysis['boundary_edges']} boundary edges")
                    return False
            
            # Check for degenerate faces
            try:
                mesh.remove_degenerate_faces()
            except:
                pass
            
            # Enhanced check: Ensure reasonable triangle count for surface detail
            expected_faces = len(mesh.vertices) * 1.8  # Higher ratio for smooth meshes
            if len(mesh.faces) < expected_faces * 0.2:  # Too few faces
                logger.warning(f"âš ï¸ Mesh has too few faces: {len(mesh.faces)} vs {expected_faces:.0f} expected")
                return False
            
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ Validation failed: {e}")
            return False
    
    def _try_alpha_shapes_fine(self, pcd) -> Optional[o3d.geometry.TriangleMesh]:
        """Fine alpha shapes for maximum detail preservation"""
        mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_alpha_shape(
            pcd, alpha=self.ballast_config['alpha_shape_alpha_fine'])
        return mesh
    
    def _try_alpha_shapes_coarse(self, pcd) -> Optional[o3d.geometry.TriangleMesh]:
        """Coarser alpha shapes as fallback"""
        mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_alpha_shape(
            pcd, alpha=self.ballast_config['alpha_shape_alpha_coarse'])
        return mesh
    
    def _try_ball_pivoting_enhanced(self, pcd) -> Optional[o3d.geometry.TriangleMesh]:
        """Enhanced ball pivoting with texture-preserving radii"""
        distances = pcd.compute_nearest_neighbor_distance()
        avg_dist = np.mean(distances)
        
        # More conservative radii for texture preservation
        factor = self.ballast_config['ball_pivot_radius_factor']
        radii = [avg_dist * factor * mult for mult in [0.5, 0.8, 1.0, 1.5, 2.0, 3.0]]
        
        mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(
            pcd, o3d.utility.DoubleVector(radii))
        return mesh
    
    def _try_poisson_ultra_high(self, pcd) -> Optional[o3d.geometry.TriangleMesh]:
        """Ultra-high quality Poisson with minimal smoothing"""
        mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(
            pcd, 
            depth=self.ballast_config['poisson_depth_ultra'],
            width=0,
            scale=1.0,
            linear_fit=False
        )
        
        # NO smoothing for texture preservation
        if len(np.asarray(mesh.vertices)) > 0:
            mesh.compute_vertex_normals()
        
        return mesh

    def _try_poisson_high_quality(self, pcd) -> Optional[o3d.geometry.TriangleMesh]:
        """Try high-quality Poisson reconstruction"""
        mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(
            pcd, 
            depth=self.ballast_config['poisson_depth_high'],
            width=0,
            scale=1.1,
            linear_fit=False
        )
        
        # Post-process for ballast
        if len(np.asarray(mesh.vertices)) > 0:
            # Light smoothing to reduce artifacts while preserving detail
            mesh = mesh.filter_smooth_simple(number_of_iterations=1)
            mesh.compute_vertex_normals()
        
        return mesh
    
    def _try_poisson_medium_quality(self, pcd) -> Optional[o3d.geometry.TriangleMesh]:
        """Try medium-quality Poisson reconstruction"""
        mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(
            pcd, 
            depth=self.ballast_config['poisson_depth_medium'],
            width=0,
            scale=1.0,
            linear_fit=False
        )
        
        if len(np.asarray(mesh.vertices)) > 0:
            mesh = mesh.filter_smooth_simple(number_of_iterations=2)
            mesh.compute_vertex_normals()
        
        return mesh
    
    def _try_poisson_low_quality(self, pcd) -> Optional[o3d.geometry.TriangleMesh]:
        """Try low-quality Poisson reconstruction as fallback"""
        mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(
            pcd, 
            depth=self.ballast_config['poisson_depth_low'],
            width=0,
            scale=1.0,
            linear_fit=False
        )
        
        if len(np.asarray(mesh.vertices)) > 0:
            mesh.compute_vertex_normals()
        
        return mesh

    def improve_normals_for_ballast(self, points: np.ndarray, normals: np.ndarray) -> np.ndarray:
        """
        Improve normal estimation specifically for rough ballast surfaces
        """
        try:
            pcd = o3d.geometry.PointCloud()
            pcd.points = o3d.utility.Vector3dVector(points)
            
            # Use more neighbors for stable normal estimation on rough surfaces
            pcd.estimate_normals(
                search_param=o3d.geometry.KDTreeSearchParamKNN(
                    knn=self.ballast_config['normal_estimation_neighbors']
                )
            )
            
            # Orient normals consistently
            pcd.orient_normals_consistent_tangent_plane(
                k=self.ballast_config['normal_estimation_neighbors']
            )
            
            improved_normals = np.asarray(pcd.normals)
            
            # Validate improved normals
            if len(improved_normals) == len(points) and not np.any(np.isnan(improved_normals)):
                return improved_normals
            else:
                return normals
                
        except Exception as e:
            logger.warning(f"âš ï¸ Normal improvement failed: {e}")
            return normals

    def _validate_texture_preservation(self, mesh: trimesh.Trimesh, original_points: np.ndarray) -> bool:
        """
        Enhanced validation focusing on texture preservation for ballast models
        """
        try:
            # Basic checks
            if len(mesh.vertices) < 4 or len(mesh.faces) < 4:
                return False
            
            # Check for reasonable simplification (allow more vertices for texture)
            vertex_ratio = len(mesh.vertices) / len(original_points)
            if vertex_ratio < 0.02:  # Less than 2% is too aggressive for ballast
                logger.warning(f"âš ï¸ Mesh too simplified: {vertex_ratio:.3f} vertex ratio")
                return False
            
            # Check bounding box preservation (more lenient for ballast)
            original_bbox = np.max(original_points, axis=0) - np.min(original_points, axis=0)
            mesh_bbox = mesh.bounds[1] - mesh.bounds[0]
            
            bbox_ratio = np.linalg.norm(mesh_bbox) / np.linalg.norm(original_bbox)
            if bbox_ratio < 0.2 or bbox_ratio > 5.0:
                logger.warning(f"âš ï¸ Mesh size changed too much: {bbox_ratio:.2f}")
                return False
            
            # Check for degenerate faces
            try:
                mesh.remove_degenerate_faces()
            except:
                pass
            
            # Enhanced check: Ensure reasonable triangle count for surface detail
            expected_faces = len(mesh.vertices) * 1.5  # Reasonable face-to-vertex ratio
            if len(mesh.faces) < expected_faces * 0.3:  # Too few faces
                logger.warning(f"âš ï¸ Mesh has too few faces for detail: {len(mesh.faces)} vs {expected_faces:.0f} expected")
                return False
            
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ Validation failed: {e}")
            return False

    def calculate_conservative_target(self, original_count: int, user_ratio: float) -> int:
        """Calculate a more conservative target that preserves ballast detail"""
        base_target = int(original_count * user_ratio)
        
        # Much more conservative minimum points for enhanced detail preservation
        if base_target < 50:
            # For very small targets, ensure enough detail
            conservative_target = max(base_target, min(50, original_count // 10))
        elif base_target < 200:
            # Small targets - add 50% more points
            conservative_target = int(base_target * 1.5)
        elif base_target < 1000:
            # Medium targets - add 30% more points  
            conservative_target = int(base_target * 1.3)
        else:
            # Large targets - add 20% more points
            conservative_target = int(base_target * 1.2)
        
        # Never exceed original count
        conservative_target = min(conservative_target, original_count)
        
        logger.info(f"ðŸŽ¯ Conservative targeting: {base_target} â†’ {conservative_target} points")
        return conservative_target


def process_single_file_worker(args):
    """Module-level worker function for parallel processing"""
    file_path, output_dir, reducer_params = args
    
    try:
        worker_reducer = BallastQualityFocusedReducer(
            target_reduction_ratio=reducer_params['target_reduction_ratio'],
            voxel_size=reducer_params['voxel_size'],
            n_cores=1,
            reconstruction_method=reducer_params['reconstruction_method'],
            fast_mode=reducer_params.get('fast_mode', False),
            use_random_forest=reducer_params.get('use_random_forest', False),
            enable_hierarchy=reducer_params.get('enable_hierarchy', True),
            hierarchy_threshold=reducer_params.get('hierarchy_threshold', 50000),
            adaptive_target=reducer_params.get('adaptive_target', False),
            enable_smoothing=reducer_params.get('enable_smoothing', True)  # NEW
        )
        
        result = worker_reducer.process_single_mesh(file_path, output_dir)
        return result
        
    except Exception as e:
        logger.error(f"Worker error processing {file_path}: {e}")
        return {'input_file': file_path, 'error': str(e)}


class BallastQualityFocusedReducer:
    """
    Enhanced Ballast Quality-Focused Reducer v2.4.0 with adaptive target finding
    
    NEW: Automatic optimal target detection when initial target fails
    """
    
    def __init__(self, 
                 target_reduction_ratio: float = 0.5,
                 voxel_size: Optional[float] = None,
                 n_cores: int = -1,
                 reconstruction_method: str = 'alpha_shapes',
                 fast_mode: bool = False,
                 use_random_forest: bool = True,
                 enable_hierarchy: bool = True,
                 force_hierarchy: bool = False,
                 hierarchy_threshold: int = 50000,
                 adaptive_target: bool = False,
                 enable_smoothing: bool = True):  # NEW parameter
        """
        Initialize Enhanced Ballast Quality-Focused Reducer v2.4.0 with adaptive targeting and smoothing
        """
        self.target_reduction_ratio = target_reduction_ratio
        self.voxel_size = voxel_size
        self.n_cores = mp.cpu_count() if n_cores == -1 else n_cores
        self.reconstruction_method = reconstruction_method
        self.fast_mode = fast_mode
        
        self.use_random_forest = use_random_forest
        self.enable_hierarchy = enable_hierarchy
        self.force_hierarchy = force_hierarchy
        self.hierarchy_threshold = hierarchy_threshold
        self.adaptive_target = adaptive_target  # Enable adaptive target finding
        self.enable_smoothing = enable_smoothing  # NEW: Enable mesh smoothing and hole filling
        
        # ENHANCED: Ballast quality specialist with superior detail preservation + adaptive targeting + smoothing
        self.ballast_specialist = BallastQualitySpecialist()
        
        # Pipeline components
        self.scaler = StandardScaler()
        self.classifier = None
        self.best_params = {}
        
        # Performance optimization caches
        self.parameter_cache = {}
        
        # Processing thresholds (more conservative for ballast)
        self.SMALL_MODEL_THRESHOLD = hierarchy_threshold // 2  # Lower threshold
        self.MEDIUM_MODEL_THRESHOLD = hierarchy_threshold
        self.LARGE_MODEL_THRESHOLD = hierarchy_threshold * 2
        self.HUGE_MODEL_THRESHOLD = hierarchy_threshold * 4
        
        logger.info(f"ðŸ—¿ Enhanced Ballast Quality-Focused Reducer v2.4.0 initialized")
        logger.info(f"   Focus: Adaptive targeting + smooth hole-free meshes + superior detail preservation")
        logger.info(f"   NEW: Adaptive target finding: {'ENABLED' if adaptive_target else 'DISABLED'}")
        logger.info(f"   NEW: Mesh smoothing & hole filling: {'ENABLED' if enable_smoothing else 'DISABLED'}")
        logger.info(f"   Features: Multi-scale analysis, zone-based processing, 8-feature extraction")
        logger.info(f"   Hierarchical processing: {'ON' if enable_hierarchy else 'OFF'}")
        logger.info(f"   Classifier: {'RandomForest' if use_random_forest else 'SVM'}")
    
    def load_mesh(self, file_path: str) -> Tuple[np.ndarray, np.ndarray]:
        """Load STL mesh and extract point cloud with normals"""
        try:
            mesh = trimesh.load(file_path)
            if hasattr(mesh, 'vertices'):
                points = np.array(mesh.vertices)
                normals = np.array(mesh.vertex_normals) if hasattr(mesh, 'vertex_normals') else None
            else:
                raise ValueError("Failed to extract vertices from mesh")
                
            if normals is None:
                o3d_mesh = o3d.io.read_triangle_mesh(file_path)
                o3d_mesh.compute_vertex_normals()
                points = np.asarray(o3d_mesh.vertices)
                normals = np.asarray(o3d_mesh.vertex_normals)
                
            logger.info(f"ðŸ“¥ Loaded mesh with {len(points):,} vertices from {Path(file_path).name}")
            return points, normals
            
        except Exception as e:
            logger.error(f"âŒ Failed to load mesh {file_path}: {e}")
            raise
    
    def normalize_points(self, points: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """Normalize points to unit cube centered at origin"""
        centroid = np.mean(points, axis=0)
        centered_points = points - centroid
        
        max_extent = np.max(np.abs(centered_points))
        scale_factor = 1.0 / max_extent if max_extent > 0 else 1.0
        normalized_points = centered_points * scale_factor
        
        normalization_params = {
            'centroid': centroid,
            'scale_factor': scale_factor
        }
        
        return normalized_points, normalization_params
    
    def denormalize_points(self, points: np.ndarray, 
                          normalization_params: Dict) -> np.ndarray:
        """Transform points back to original coordinate frame"""
        if len(points) == 0:
            return points
        denormalized = points / normalization_params['scale_factor']
        denormalized += normalization_params['centroid']
        return denormalized
    
    @time_function
    def train_classifier(self, features: np.ndarray, labels: np.ndarray):
        """Train classifier optimized for ballast features"""
        features_scaled = self.scaler.fit_transform(features)
        
        if self.use_random_forest:
            # Optimized RandomForest for ballast
            self.classifier = RandomForestClassifier(
                n_estimators=100,  # More trees for ballast
                max_depth=15,      # Deeper trees
                random_state=42, 
                n_jobs=min(4, self.n_cores)
            )
        else:
            # SVM for quality
            self.classifier = SVC(kernel='rbf', probability=True, random_state=42)
        
        self.classifier.fit(features_scaled, labels)
        
        train_accuracy = self.classifier.score(features_scaled, labels)
        classifier_type = "RandomForest" if self.use_random_forest else "SVM"
        logger.debug(f"âœ… {classifier_type} training accuracy: {train_accuracy:.3f}")
    
    def knn_reinforcement(self, points: np.ndarray, important_mask: np.ndarray, 
                         k_neighbors: int) -> np.ndarray:
        """Conservative KNN reinforcement for ballast"""
        if np.sum(important_mask) == 0:
            return important_mask
        
        important_indices = np.where(important_mask)[0]
        reinforced_mask = important_mask.copy()
        
        k = min(k_neighbors, len(points)-1)
        nbrs = NearestNeighbors(n_neighbors=k, algorithm='kd_tree')
        nbrs.fit(points)
        
        # Process all important points for quality (don't limit for ballast)
        for idx in important_indices:
            distances, neighbor_indices = nbrs.kneighbors([points[idx]])
            reinforced_mask[neighbor_indices[0]] = True
        
        logger.debug(f"ðŸ”— Conservative KNN: {np.sum(important_mask):,} â†’ {np.sum(reinforced_mask):,} points")
        return reinforced_mask
    
    def radius_merge(self, points: np.ndarray, normals: np.ndarray, 
                    epsilon: float) -> Tuple[np.ndarray, np.ndarray]:
        """Conservative radius merge for ballast quality"""
        if epsilon <= 0 or len(points) == 0:
            return points, normals
        
        clustering = DBSCAN(eps=epsilon, min_samples=1)
        cluster_labels = clustering.fit_predict(points)
        
        unique_labels = np.unique(cluster_labels)
        merged_points = []
        merged_normals = []
        
        for label in unique_labels:
            if label == -1:
                # Keep noise points for ballast detail
                noise_indices = np.where(cluster_labels == label)[0]
                merged_points.extend(points[noise_indices])
                merged_normals.extend(normals[noise_indices])
            else:
                cluster_indices = np.where(cluster_labels == label)[0]
                
                # For ballast, be more conservative about merging
                if len(cluster_indices) <= 3:  # Keep small clusters as-is
                    merged_points.extend(points[cluster_indices])
                    merged_normals.extend(normals[cluster_indices])
                else:
                    # Only merge larger clusters
                    centroid = np.mean(points[cluster_indices], axis=0)
                    avg_normal = np.mean(normals[cluster_indices], axis=0)
                    norm = np.linalg.norm(avg_normal)
                    if norm > 0:
                        avg_normal /= norm
                    
                    merged_points.append(centroid)
                    merged_normals.append(avg_normal)
        
        merged_points = np.array(merged_points) if merged_points else np.empty((0, 3))
        merged_normals = np.array(merged_normals) if merged_normals else np.empty((0, 3))
        
        logger.debug(f"ðŸ”„ Conservative merge: {len(points):,} â†’ {len(merged_points):,} points")
        return merged_points, merged_normals
    
    def dbscan_cleanup(self, points: np.ndarray, normals: np.ndarray,
                      eps: float, min_samples: int = 2) -> Tuple[np.ndarray, np.ndarray]:
        """Very conservative DBSCAN cleanup for ballast"""
        if len(points) == 0:
            return points, normals
        
        # Use very conservative parameters for ballast
        clustering = DBSCAN(eps=eps, min_samples=max(1, min_samples//2))  # More lenient
        cluster_labels = clustering.fit_predict(points)
        
        valid_mask = cluster_labels != -1
        
        # If too many points removed, keep them
        if np.sum(valid_mask) < len(points) * 0.7:  # If losing more than 30%
            logger.debug(f"ðŸ›¡ï¸ Keeping outliers to preserve ballast detail")
            return points, normals
        
        if np.sum(valid_mask) == 0:
            return points, normals
        
        logger.debug(f"ðŸ§¹ Conservative cleanup: {len(points):,} â†’ {np.sum(valid_mask):,} points")
        return points[valid_mask], normals[valid_mask]
    
    def process_ballast_quality_focused_with_adaptive_target(self, points: np.ndarray, normals: np.ndarray, 
                                                           input_path: str) -> Tuple[np.ndarray, np.ndarray, Dict]:
        """
        NEW: Enhanced ballast processing with adaptive target finding, vertex prediction, and superior detail preservation
        """
        logger.info("ðŸ—¿ Starting ENHANCED ballast processing with ADAPTIVE TARGET FINDING & VERTEX PREDICTION...")
        
        # Enhanced ballast complexity analysis with multi-scale surface understanding
        analysis = self.ballast_specialist.analyze_ballast_complexity(points)
        surface_analysis = analysis['surface_analysis']
        
        # Store original mesh information for vertex calculations
        original_count = len(points)
        original_vertices = original_count  # Points from mesh vertices
        
        # Calculate initial user target
        user_target = int(original_count * self.target_reduction_ratio)
        
        # NEW: Predict vertex outcomes for different reconstruction methods
        logger.info("ðŸ“Š PREDICTING VERTEX OUTCOMES...")
        vertex_predictions = {}
        for method in ['alpha_shapes', 'ball_pivoting', 'poisson']:
            vertex_predictions[method] = self.ballast_specialist.predict_vertex_outcomes(
                user_target, method, analysis)
        
        # Show best prediction for current method
        current_prediction = vertex_predictions.get(self.reconstruction_method, 
                                                   vertex_predictions['alpha_shapes'])
        logger.info(f"ðŸŽ¯ Using {self.reconstruction_method} reconstruction:")
        logger.info(f"   Expected vertex range: {current_prediction['vertex_estimates']['expected_range']}")
        logger.info(f"   Quality level: {current_prediction['quality_summary']['expected_detail_level']}")
        
        # NEW: Use adaptive target finder to get optimal target points
        if self.adaptive_target:
            logger.info("ðŸ” ADAPTIVE TARGET FINDING: Analyzing optimal point count...")
            adaptive_result = self.ballast_specialist.find_adaptive_target_points(
                points, user_target, analysis, self.adaptive_target)
            
            optimal_target = adaptive_result['target']
            adjustment_info = adaptive_result
            
            # Update vertex predictions for adaptive target
            adaptive_prediction = self.ballast_specialist.predict_vertex_outcomes(
                optimal_target, self.reconstruction_method, analysis)
            
            logger.info(f"ðŸŽ¯ ADAPTIVE TARGET RESULT:")
            logger.info(f"   Strategy: {adaptive_result['strategy']}")
            logger.info(f"   User target: {user_target:,} â†’ Adaptive target: {optimal_target:,}")
            logger.info(f"   Adjustment factor: {adaptive_result['adjustment_factor']:.1f}x")
            logger.info(f"   Updated vertex prediction: {adaptive_prediction['vertex_estimates']['expected_range']}")
            logger.info(f"   Reason: {adaptive_result['reason']}")
        else:
            # Use original quality-focused targeting
            optimal_target = self.ballast_specialist.get_quality_focused_target_points(
                points, self.target_reduction_ratio, analysis)
            adjustment_info = {
                'target': optimal_target,
                'strategy': 'quality_focused_original',
                'reason': 'Adaptive targeting disabled'
            }
        
        # Adjust target ratio based on adaptive/quality requirements
        adjusted_ratio = optimal_target / len(points)
        original_ratio = self.target_reduction_ratio
        self.target_reduction_ratio = adjusted_ratio
        
        logger.info(f"ðŸŽ¯ Final target adjustment: {original_ratio:.4f} â†’ {adjusted_ratio:.4f}")
        logger.info(f"   Points target: {len(points):,} â†’ {optimal_target:,}")
        
        # Get quality-focused parameters that respect the adaptive target
        ballast_params = self.ballast_specialist.get_ballast_quality_parameters(
            analysis, optimal_target, len(points), aggressive_reduction=(adjusted_ratio < 0.02))
        
        # Normalize points
        normalized_points, norm_params = self.normalize_points(points)
        
        # Process with retry logic for failed targets
        max_retries = 3
        current_target = optimal_target
        retry_count = 0
        final_points = None
        final_normals = None
        
        while retry_count < max_retries:
            try:
                logger.info(f"ðŸ”„ Processing attempt {retry_count + 1}/{max_retries} with target: {current_target:,}")
                
                # Update vertex prediction for current attempt
                attempt_prediction = self.ballast_specialist.predict_vertex_outcomes(
                    current_target, self.reconstruction_method, analysis)
                logger.info(f"   Expected vertices this attempt: {attempt_prediction['vertex_estimates']['quality_adjusted_vertices']:,}")
                
                # Recalculate parameters for current target
                current_ratio = current_target / len(points)
                current_params = self.ballast_specialist.get_ballast_quality_parameters(
                    analysis, current_target, len(points), aggressive_reduction=(current_ratio < 0.02))
                
                # ENHANCED: Multi-scale feature extraction for ballast (8 features)
                logger.info("ðŸ”¬ Enhanced 8-feature extraction with multi-scale analysis...")
                features = self.ballast_specialist.enhanced_feature_extraction_for_ballast(
                    normalized_points, surface_analysis, k_neighbors=current_params['k_neighbors'])
                
                # ENHANCED: Multi-pass importance detection with zone-specific processing
                logger.info("ðŸŽ¯ Multi-pass importance detection with zone-based processing...")
                pseudo_labels = self.ballast_specialist.create_ballast_importance_labels(
                    features, normalized_points, surface_analysis, current_params['importance_threshold'])
                
                # Train classifier
                self.train_classifier(features, pseudo_labels)
                
                # Predict importance
                features_scaled = self.scaler.transform(features)
                important_probs = self.classifier.predict_proba(features_scaled)[:, 1]
                important_mask = important_probs > 0.25  # Lower threshold for ballast detail preservation
                
                logger.info(f"ðŸŽ¯ Initial important points: {np.sum(important_mask):,}/{len(points):,}")
                
                # Conservative KNN reinforcement
                reinforced_mask = self.knn_reinforcement(
                    normalized_points, important_mask, current_params['k_neighbors'])
                
                logger.info(f"ðŸ”— After reinforcement: {np.sum(reinforced_mask):,} points")
                
                selected_points = normalized_points[reinforced_mask]
                selected_normals = normals[reinforced_mask]
                
                if len(selected_points) == 0:
                    raise ValueError("No points selected after processing")
                
                # ENHANCED: Adaptive clustering based on surface complexity zones
                logger.info("ðŸ”§ Enhanced adaptive clustering with detail zones...")
                merged_points, merged_normals = self._adaptive_clustering_enhanced(
                    selected_points, selected_normals, surface_analysis, reinforced_mask)
                
                logger.info(f"ðŸ”„ After enhanced clustering: {len(merged_points):,} points")
                
                # Very conservative cleanup
                final_points, final_normals = self.dbscan_cleanup(
                    merged_points, merged_normals, current_params['dbscan_eps'])
                
                logger.info(f"ðŸ§¹ After cleanup: {len(final_points):,} points")
                
                # Check if we have enough points for reconstruction
                if len(final_points) < 20:
                    raise ValueError(f"Too few points after processing: {len(final_points)}")
                
                # Denormalize
                final_points = self.denormalize_points(final_points, norm_params)
                
                # Success!
                logger.info(f"âœ… Processing successful with {len(final_points):,} points")
                break
                
            except Exception as e:
                logger.warning(f"âš ï¸ Processing attempt {retry_count + 1} failed: {e}")
                retry_count += 1
                
                if retry_count < max_retries:
                    # Increase target for next attempt (make less aggressive)
                    current_target = int(current_target * 1.5)
                    current_target = min(current_target, len(points) // 2)  # Cap at 50% of original
                    logger.info(f"ðŸ”„ Retrying with increased target: {current_target:,}")
                else:
                    logger.error(f"âŒ All processing attempts failed, using fallback")
                    # Emergency fallback
                    step = max(1, len(points) // max(100, optimal_target // 2))
                    final_points = points[::step]
                    final_normals = normals[::step]
        
        # Restore original ratio
        self.target_reduction_ratio = original_ratio
        
        method_info = {
            'processing_method': 'enhanced_ballast_quality_focused_v2.4.0_adaptive_with_vertex_prediction_and_smoothing',
            'ballast_analysis': analysis,
            'ballast_parameters': ballast_params,
            'surface_analysis': surface_analysis,
            'adaptive_target_info': adjustment_info,
            'vertex_predictions': vertex_predictions,
            'final_vertex_prediction': self.ballast_specialist.predict_vertex_outcomes(
                len(final_points), self.reconstruction_method, analysis),
            'target_adjustment': {
                'original_ratio': original_ratio,
                'adjusted_ratio': adjusted_ratio,
                'original_target': user_target,
                'adaptive_target': optimal_target,
                'final_count': len(final_points),
                'retry_count': retry_count
            },
            'detail_preservation_features': {
                'multi_scale_analysis': True,
                'zone_based_processing': True,
                'enhanced_8_features': True,
                'texture_preserving_reconstruction': True,
                'adaptive_target_finding': self.adaptive_target,
                'vertex_prediction_and_calculation': True,
                'mesh_smoothing_and_hole_filling': self.enable_smoothing  # NEW
            }
        }
        
        return final_points, final_normals, method_info
    
    def _adaptive_clustering_enhanced(self, points: np.ndarray, normals: np.ndarray, 
                                    surface_analysis: Dict, point_mask: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Enhanced adaptive clustering based on surface complexity zones"""
        zones = surface_analysis.get('complexity_zones', {})
        
        if not zones:
            # Fallback to standard clustering
            epsilon = 0.010  # Conservative default
            return self.radius_merge(points, normals, epsilon)
        
        clustered_points = []
        clustered_normals = []
        
        # Map points back to original indices for zone lookup
        original_indices = np.where(point_mask)[0]
        
        # Process each complexity zone with appropriate parameters
        processed_indices = set()
        
        for zone_name, zone_indices in zones.items():
            if len(zone_indices) == 0:
                continue
            
            # Find intersection of selected points with this zone
            zone_in_selection = []
            local_indices = []
            
            for i, orig_idx in enumerate(original_indices):
                if orig_idx in zone_indices and orig_idx not in processed_indices:
                    zone_in_selection.append(i)
                    local_indices.append(orig_idx)
                    processed_indices.add(orig_idx)
            
            if not zone_in_selection:
                continue
            
            zone_points = points[zone_in_selection]
            zone_normals = normals[zone_in_selection]
            
            # Select epsilon based on zone complexity
            if zone_name == 'high_detail':
                epsilon = 0.002  # Very fine clustering
            elif zone_name == 'medium_detail':
                epsilon = 0.005  # Medium clustering
            else:  # low_detail
                epsilon = 0.010  # Coarser clustering
            
            # Apply clustering to zone
            zone_clustered_points, zone_clustered_normals = self.radius_merge(
                zone_points, zone_normals, epsilon)
            
            if len(zone_clustered_points) > 0:
                clustered_points.append(zone_clustered_points)
                clustered_normals.append(zone_clustered_normals)
            
            logger.debug(f"ðŸ”§ Enhanced {zone_name}: {len(zone_points)} â†’ {len(zone_clustered_points)} points")
        
        # Handle any unprocessed points
        unprocessed_indices = []
        for i, orig_idx in enumerate(original_indices):
            if orig_idx not in processed_indices:
                unprocessed_indices.append(i)
        
        if unprocessed_indices:
            remaining_points = points[unprocessed_indices]
            remaining_normals = normals[unprocessed_indices]
            remaining_clustered_points, remaining_clustered_normals = self.radius_merge(
                remaining_points, remaining_normals, 0.008)
            
            if len(remaining_clustered_points) > 0:
                clustered_points.append(remaining_clustered_points)
                clustered_normals.append(remaining_clustered_normals)
        
        if not clustered_points:
            return points, normals
        
        # Combine results
        final_points = np.vstack(clustered_points)
        final_normals = np.vstack(clustered_normals)
        
        logger.info(f"ðŸ”§ Enhanced adaptive clustering: {len(points)} â†’ {len(final_points)} points")
        return final_points, final_normals
        
    def ensure_target_compliance(self, points: np.ndarray, normals: np.ndarray, 
                               target_points: int, user_target: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        Ensure we actually hit close to the user's target while maintaining quality
        """
        current_count = len(points)
        
        # If we're way over the quality-adjusted target, need to reduce further
        if current_count > target_points * 2:  # More than 2x over target
            logger.info(f"ðŸŽ¯ Too many points ({current_count:,}), reducing to target ({target_points:,})")
            
            # Use importance-based sampling to get closer to target
            if current_count > target_points:
                # Calculate features for final selection
                features = self.ballast_specialist.enhanced_feature_extraction_for_ballast(
                    points, {'detail_map': np.ones(len(points))}, k_neighbors=8)
                
                # Calculate importance scores
                importance_scores = (
                    features[:, 2] * 2.0 +  # Surface variation (most important for ballast)
                    features[:, 3] * 1.5 +  # Max neighbor distance  
                    features[:, 4] * 1.5 +  # Curvature
                    features[:, 5] * 1.0 +  # Surface roughness
                    features[:, 0] * 0.3 +  # Centroid distance (less important)
                    (1.0 / (features[:, 1] + 1e-8)) * 0.5  # Boundary points
                )
                
                # Select top most important points
                top_indices = np.argsort(importance_scores)[-target_points:]
                selected_points = points[top_indices]
                selected_normals = normals[top_indices]
                
                logger.info(f"âœ‚ï¸ Importance-based reduction: {current_count:,} â†’ {len(selected_points):,} points")
                return selected_points, selected_normals
        
        # If we're close to target, keep as-is
        logger.info(f"âœ… Point count acceptable: {current_count:,} points (target: {target_points:,})")
        return points, normals
    
    def process_single_mesh(self, input_path: str, output_dir: str) -> Dict:
        """
        Main processing method with enhanced ballast quality focus and adaptive targeting
        """
        try:
            start_time = time.time()
            
            # Check if this is a ballast model
            is_ballast = self.ballast_specialist.detect_ballast_model(input_path)
            
            # Load mesh
            load_start = time.time()
            points, normals = self.load_mesh(input_path)
            original_count = len(points)
            load_time = time.time() - load_start
            
            if original_count == 0:
                return {'input_file': input_path, 'error': 'Empty mesh'}
            
            logger.info(f"â±ï¸  Mesh loaded in {load_time:.1f}s")
            
            if is_ballast:
                adaptive_status = "ENABLED" if self.adaptive_target else "DISABLED"
                logger.info(f"ðŸ—¿ BALLAST MODEL DETECTED - Enhanced processing with adaptive targeting {adaptive_status}")
            else:
                logger.info(f"ðŸ“„ Regular model - Using standard processing")
            
            # Process based on type
            processing_start = time.time()
            
            if is_ballast:
                # Use ENHANCED quality-focused processing with adaptive targeting for ballast
                final_points, final_normals, method_info = self.process_ballast_quality_focused_with_adaptive_target(
                    points, normals, input_path)
            else:
                # Use standard processing for non-ballast
                # (Simplified version for non-ballast models)
                normalized_points, norm_params = self.normalize_points(points)
                
                # Simple uniform sampling for non-ballast
                target_count = max(100, int(len(points) * self.target_reduction_ratio))
                step = max(1, len(points) // target_count)
                sampled_indices = np.arange(0, len(points), step)
                
                final_points = points[sampled_indices]
                final_normals = normals[sampled_indices]
                
                method_info = {
                    'processing_method': 'standard_uniform_sampling',
                    'target_count': target_count,
                    'final_count': len(final_points)
                }
            
            processing_time = time.time() - processing_start
            logger.info(f"â±ï¸  Processing completed in {processing_time:.1f}s")
            
            # ENHANCED: Texture-preserving surface reconstruction for ballast with smoothing and hole filling
            recon_start = time.time()
            if is_ballast:
                smoothing_status = "ENABLED" if self.enable_smoothing else "DISABLED"
                logger.info(f"ðŸ”§ Applying ENHANCED surface reconstruction (smoothing: {smoothing_status})")
                reconstructed_mesh = self.ballast_specialist.quality_focused_surface_reconstruction(
                    final_points, final_normals, self.reconstruction_method, self.enable_smoothing)
                
                # NEW: Calculate comprehensive vertex statistics with smoothness analysis
                if reconstructed_mesh:
                    vertex_stats = self.ballast_specialist.calculate_final_vertex_statistics(
                        original_count, original_count, len(final_points), reconstructed_mesh)
                    method_info['vertex_statistics'] = vertex_stats
                    
                    # NEW: Add mesh quality analysis
                    if self.enable_smoothing:
                        mesh_quality = self.ballast_specialist.mesh_smoother.detect_mesh_holes(reconstructed_mesh)
                        method_info['mesh_quality'] = mesh_quality
                        
                        logger.info("ðŸ“Š COMPREHENSIVE MESH ANALYSIS:")
                        logger.info(f"   ðŸ”¢ Vertex reduction: {vertex_stats.get('vertex_reduction_percentage', 0):.1f}%")
                        logger.info(f"   ðŸ—œï¸ Compression factor: {vertex_stats.get('vertex_compression_factor', 0):.1f}x")
                        logger.info(f"   ðŸ’¾ Memory reduction: {vertex_stats['memory_reduction']['reduction_percentage']:.1f}%")
                        logger.info(f"   ðŸŽ¯ Vertex density: {vertex_stats.get('final_vertex_to_point_ratio', 0):.2f}")
                        logger.info(f"   âœ¨ Mesh quality score: {mesh_quality['quality_score']:.2f}")
                        logger.info(f"   ðŸ”§ Watertight: {'YES' if mesh_quality['is_watertight'] else 'NO'}")
                        logger.info(f"   ðŸ•³ï¸ Boundary edges: {mesh_quality['boundary_edges']}")
                    else:
                        logger.info("ðŸ“Š COMPREHENSIVE VERTEX ANALYSIS:")
                        logger.info(f"   ðŸ”¢ Vertex reduction: {vertex_stats.get('vertex_reduction_percentage', 0):.1f}%")
                        logger.info(f"   ðŸ—œï¸ Compression factor: {vertex_stats.get('vertex_compression_factor', 0):.1f}x")
                        logger.info(f"   ðŸ’¾ Memory reduction: {vertex_stats['memory_reduction']['reduction_percentage']:.1f}%")
                        logger.info(f"   ðŸŽ¯ Vertex density: {vertex_stats.get('final_vertex_to_point_ratio', 0):.2f}")
                        if vertex_stats.get('mesh_watertight'):
                            logger.info(f"   âœ… Mesh is watertight")
                        else:
                            logger.info(f"   âš ï¸ Mesh may have holes")
            else:
                # Standard reconstruction for non-ballast
                reconstructed_mesh = self._standard_reconstruction(final_points, final_normals)
            
            recon_time = time.time() - recon_start
            logger.info(f"â±ï¸  Surface reconstruction in {recon_time:.1f}s")
            
            # Save results
            save_start = time.time()
            filename = Path(input_path).stem
            model_output_dir = Path(output_dir) / filename
            model_output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"ðŸ“ Created subfolder: {model_output_dir.name}/")
            
            # Save simplified STL
            stl_path = None
            if reconstructed_mesh:
                stl_path = model_output_dir / f"{filename}_enhanced.stl"
                reconstructed_mesh.export(str(stl_path))
                quality_indicator = " (ENHANCED adaptive detail-preserving)" if is_ballast else ""
                logger.info(f"ðŸ’¾ Saved STL{quality_indicator}: {model_output_dir.name}/{filename}_enhanced.stl")
            else:
                logger.warning(f"âš ï¸  No STL generated for {filename} (reconstruction failed)")
            
            # Save point cloud as CSV
            csv_path = model_output_dir / f"{filename}_points.csv"
            point_df = pd.DataFrame(final_points, columns=['x', 'y', 'z'])
            normal_df = pd.DataFrame(final_normals, columns=['nx', 'ny', 'nz'])
            combined_df = pd.concat([point_df, normal_df], axis=1)
            combined_df.to_csv(csv_path, index=False)
            logger.info(f"ðŸ’¾ Saved CSV: {model_output_dir.name}/{filename}_points.csv")
            
            # Save as DAT file
            dat_path = model_output_dir / f"{filename}_points.dat"
            np.savetxt(dat_path, final_points, fmt='%.6f')
            logger.info(f"ðŸ’¾ Saved DAT: {model_output_dir.name}/{filename}_points.dat")
            
            save_time = time.time() - save_start
            total_time = time.time() - start_time
            
            # Final completion message
            processing_type = "ENHANCED Adaptive ballast" if is_ballast else "Standard"
            logger.info(f"âœ… COMPLETED: {filename} â†’ All files saved to {model_output_dir.name}/")
            logger.info(f"ðŸ“Š Summary: {original_count:,} â†’ {len(final_points):,} points (ratio: {len(final_points) / original_count:.4f})")
            logger.info(f"â±ï¸  Total processing time: {total_time:.1f}s")
            logger.info(f"ðŸš€ Method: {processing_type}")
            
            if is_ballast:
                if 'adaptive_target_info' in method_info:
                    adaptive_info = method_info['adaptive_target_info']
                    logger.info(f"ðŸŽ¯ Adaptive targeting: {adaptive_info['strategy']}")
                    if 'adjustment_factor' in adaptive_info:
                        logger.info(f"   Adjustment: {adaptive_info['adjustment_factor']:.1f}x user target")
                
                # NEW: Show vertex statistics if available
                if 'vertex_statistics' in method_info:
                    vertex_stats = method_info['vertex_statistics']
                    logger.info(f"ðŸ“Š Vertex Statistics:")
                    logger.info(f"   Vertices: {vertex_stats['original_vertices']:,} â†’ {vertex_stats['final_vertices']:,}")
                    logger.info(f"   Vertex reduction: {vertex_stats.get('vertex_reduction_percentage', 0):.1f}%")
                    logger.info(f"   Memory saved: {vertex_stats['memory_reduction']['reduction_mb']:.1f} MB")
                
                # Show vertex predictions vs actual
                if 'vertex_predictions' in method_info and reconstructed_mesh:
                    current_method_prediction = method_info['vertex_predictions'].get(self.reconstruction_method, {})
                    if 'vertex_estimates' in current_method_prediction:
                        predicted = current_method_prediction['vertex_estimates']['quality_adjusted_vertices']
                        actual = len(reconstructed_mesh.vertices)
                        accuracy = abs(predicted - actual) / predicted * 100 if predicted > 0 else 0
                        logger.info(f"ðŸ”® Prediction accuracy: {100 - accuracy:.1f}% (predicted {predicted:,}, actual {actual:,})")
                
                if 'target_adjustment' in method_info:
                    adj = method_info['target_adjustment']
                    if adj.get('retry_count', 0) > 0:
                        logger.info(f"ðŸ”„ Required {adj['retry_count']} retry attempts")
                
                detail_features = method_info.get('detail_preservation_features', {})
                logger.info(f"âœ¨ Enhanced features applied:")
                if detail_features.get('multi_scale_analysis'):
                    logger.info(f"   âœ… Multi-scale surface analysis")
                if detail_features.get('zone_based_processing'):
                    logger.info(f"   âœ… Zone-based adaptive processing")
                if detail_features.get('enhanced_8_features'):
                    logger.info(f"   âœ… Enhanced 8-feature extraction")
                if detail_features.get('texture_preserving_reconstruction'):
                    logger.info(f"   âœ… Texture-preserving reconstruction")
                if detail_features.get('adaptive_target_finding'):
                    logger.info(f"   âœ… NEW: Adaptive target finding")
                if detail_features.get('vertex_prediction_and_calculation'):
                    logger.info(f"   âœ… NEW: Vertex prediction & calculation")
                if detail_features.get('mesh_smoothing_and_hole_filling'):
                    logger.info(f"   âœ… NEW: Mesh smoothing & hole filling")
            
            logger.info("-" * 80)
            
            # Results summary with comprehensive vertex information
            results = {
                'input_file': input_path,
                'original_points': original_count,
                'final_points': len(final_points),
                'reduction_ratio': len(final_points) / original_count,
                'target_ratio': self.target_reduction_ratio,
                'processing_time': total_time,
                'time_breakdown': {
                    'load': load_time,
                    'processing': processing_time,
                    'reconstruction': recon_time,
                    'save': save_time
                },
                'method_info': method_info,
                'ballast_detected': is_ballast,
                'quality_focused': is_ballast,
                'adaptive_target_enabled': self.adaptive_target,
                'output_files': {
                    'stl': str(stl_path) if stl_path else None,
                    'csv': str(csv_path),
                    'dat': str(dat_path)
                }
            }
            
            # Add vertex information if available
            if reconstructed_mesh:
                results['final_vertices'] = len(reconstructed_mesh.vertices)
                results['final_faces'] = len(reconstructed_mesh.faces)
                results['vertex_reduction_ratio'] = len(reconstructed_mesh.vertices) / original_count
                
                # Add comprehensive vertex statistics for ballast models
                if is_ballast and 'vertex_statistics' in method_info:
                    results['vertex_statistics'] = method_info['vertex_statistics']
                    results['vertex_predictions'] = method_info.get('vertex_predictions', {})
                    
                    # Calculate prediction accuracy
                    if 'vertex_predictions' in method_info:
                        current_method_prediction = method_info['vertex_predictions'].get(self.reconstruction_method, {})
                        if 'vertex_estimates' in current_method_prediction:
                            predicted = current_method_prediction['vertex_estimates']['quality_adjusted_vertices']
                            actual = len(reconstructed_mesh.vertices)
                            results['vertex_prediction_accuracy'] = (
                                100 - abs(predicted - actual) / predicted * 100) if predicted > 0 else 0
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Failed to process {input_path}: {e}")
            return {'input_file': input_path, 'error': str(e)}
    
    def _standard_reconstruction(self, points: np.ndarray, normals: np.ndarray) -> Optional[trimesh.Trimesh]:
        """Standard surface reconstruction for non-ballast models"""
        if len(points) < 4:
            return None
            
        try:
            pcd = o3d.geometry.PointCloud()
            pcd.points = o3d.utility.Vector3dVector(points)
            pcd.normals = o3d.utility.Vector3dVector(normals)
            
            mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(
                pcd, depth=8, width=0, scale=1.1, linear_fit=False)
            
            vertices = np.asarray(mesh.vertices)
            faces = np.asarray(mesh.triangles)
            
            if len(faces) == 0:
                return None
            
            return trimesh.Trimesh(vertices=vertices, faces=faces)
            
        except Exception as e:
            logger.error(f"âŒ Standard reconstruction failed: {e}")
            return None
    
    def process_batch(self, input_dir: str, output_dir: str, 
                     file_pattern: str = "*.stl") -> List[Dict]:
        """Process multiple mesh files in parallel"""
        input_path = Path(input_dir)
        stl_files = list(input_path.glob(file_pattern))
        
        if not stl_files:
            logger.warning(f"âŒ No STL files found in {input_dir}")
            return []
        
        adaptive_status = "ENABLED" if self.adaptive_target else "DISABLED"
        logger.info(f"ðŸš€ Processing {len(stl_files)} files with {self.n_cores} cores")
        logger.info(f"ðŸ—¿ Enhanced ballast quality-focused processing enabled")
        logger.info(f"ðŸŽ¯ NEW: Adaptive target finding {adaptive_status}")
        
        if len(stl_files) == 1 or self.n_cores == 1:
            # Single file or single core
            results = []
            for file_path in stl_files:
                try:
                    result = self.process_single_mesh(str(file_path), output_dir)
                    results.append(result)
                except Exception as e:
                    logger.error(f"âŒ Error processing {file_path}: {e}")
                    results.append({'input_file': str(file_path), 'error': str(e)})
        else:
            # Parallel processing
            reducer_params = {
                'target_reduction_ratio': self.target_reduction_ratio,
                'voxel_size': self.voxel_size,
                'reconstruction_method': self.reconstruction_method,
                'fast_mode': self.fast_mode,
                'use_random_forest': self.use_random_forest,
                'enable_hierarchy': self.enable_hierarchy,
                'hierarchy_threshold': self.hierarchy_threshold,
                'adaptive_target': self.adaptive_target,
                'enable_smoothing': self.enable_smoothing  # NEW
            }
            
            worker_args = [
                (str(file_path), output_dir, reducer_params) 
                for file_path in stl_files
            ]
            
            with ProcessPoolExecutor(max_workers=self.n_cores) as executor:
                logger.info(f"ðŸš€ Starting parallel processing with {self.n_cores} workers...")
                
                future_to_args = {
                    executor.submit(process_single_file_worker, args): args[0]
                    for args in worker_args
                }
                
                results = []
                completed = 0
                total_files = len(stl_files)
                
                for future in as_completed(future_to_args):
                    try:
                        result = future.result(timeout=600)
                        results.append(result)
                        completed += 1
                        
                        if 'error' not in result:
                            filename = Path(result['input_file']).name
                            model_name = Path(result['input_file']).stem
                            method = result.get('method_info', {}).get('processing_method', 'unknown')
                            ballast_detected = result.get('ballast_detected', False)
                            quality_focused = result.get('quality_focused', False)
                            adaptive_enabled = result.get('adaptive_target_enabled', False)
                            
                            status_icons = ""
                            if ballast_detected:
                                status_icons += "ðŸ—¿"
                            if quality_focused:
                                status_icons += "âœ¨"
                            if adaptive_enabled:
                                status_icons += "ðŸŽ¯"
                            
                            logger.info(f"ðŸŽ‰ [{completed}/{total_files}] COMPLETED: {filename} {status_icons}")
                            logger.info(f"   ðŸ“ Subfolder: {model_name}/")
                            logger.info(f"   ðŸ“Š Points: {result['original_points']:,} â†’ {result['final_points']:,}")
                            
                            # Show vertex information if available
                            if 'final_vertices' in result:
                                logger.info(f"   ðŸ“ Vertices: {result['original_points']:,} â†’ {result['final_vertices']:,}")
                                logger.info(f"   ðŸ—œï¸ Vertex reduction: {((1 - result['vertex_reduction_ratio']) * 100):.1f}%")
                            
                            logger.info(f"   ðŸš€ Method: {method}")
                            logger.info(f"   â±ï¸  Time: {result['processing_time']:.1f}s")
                            if ballast_detected:
                                logger.info(f"   ðŸ—¿ Enhanced ballast processing: Applied")
                            if adaptive_enabled:
                                logger.info(f"   ðŸŽ¯ NEW: Adaptive target finding: Applied")
                            
                            # Show vertex prediction accuracy for ballast models
                            if 'vertex_prediction_accuracy' in result:
                                logger.info(f"   ðŸ”® Vertex prediction accuracy: {result['vertex_prediction_accuracy']:.1f}%")
                            
                            logger.info("-" * 60)
                        else:
                            filename = Path(result['input_file']).name
                            logger.error(f"âŒ [{completed}/{total_files}] FAILED: {filename}")
                            logger.error(f"   ðŸš¨ Error: {result['error']}")
                            logger.error("-" * 60)
                            
                    except Exception as e:
                        file_path = future_to_args[future]
                        logger.error(f"âŒ Future error for {file_path}: {e}")
                        results.append({'input_file': str(file_path), 'error': str(e)})
                        completed += 1
        
        # Save batch summary
        summary_path = Path(output_dir) / "batch_summary.csv"
        summary_df = pd.DataFrame(results)
        summary_df.to_csv(summary_path, index=False)
        
        logger.info("=" * 80)
        logger.info(f"ðŸ“‹ BATCH SUMMARY SAVED: {summary_path.name}")
        logger.info(f"ðŸ“Š Total files processed: {len(results)}")
        successful_count = len([r for r in results if 'error' not in r])
        failed_count = len([r for r in results if 'error' in r])
        logger.info(f"âœ… Successful: {successful_count}")
        logger.info(f"âŒ Failed: {failed_count}")
        
        if successful_count > 0:
            # Performance analysis
            successful_results = [r for r in results if 'error' not in r]
            ballast_count = len([r for r in successful_results if r.get('ballast_detected', False)])
            adaptive_count = len([r for r in successful_results if r.get('adaptive_target_enabled', False)])
            standard_count = successful_count - ballast_count
            
            if ballast_count > 0:
                logger.info(f"ðŸ—¿ Enhanced ballast processing: {ballast_count} files")
                ballast_results = [r for r in successful_results if r.get('ballast_detected', False)]
                avg_ballast_time = np.mean([r['processing_time'] for r in ballast_results])
                avg_ballast_ratio = np.mean([r['reduction_ratio'] for r in ballast_results])
                logger.info(f"   Average processing time: {avg_ballast_time:.1f}s")
                logger.info(f"   Average reduction ratio: {avg_ballast_ratio:.4f}")
            
            if adaptive_count > 0:
                logger.info(f"ðŸŽ¯ NEW: Adaptive target finding: {adaptive_count} files")
            
            if standard_count > 0:
                logger.info(f"ðŸ“ Standard processing: {standard_count} files")
            
            avg_time = np.mean([r['processing_time'] for r in successful_results])
            logger.info(f"â±ï¸  Overall average time: {avg_time:.1f}s per file")
        
        logger.info("=" * 80)
        return results


def estimate_target_ratio(input_path: str, target_count: int) -> float:
    """Enhanced target ratio estimation"""
    try:
        reducer = BallastQualityFocusedReducer()
        
        if os.path.isfile(input_path):
            sample_file = input_path
        else:
            stl_files = list(Path(input_path).glob("*.stl"))
            if not stl_files:
                return 0.5
            sample_file = str(stl_files[0])
        
        points, _ = reducer.load_mesh(sample_file)
        original_count = len(points)
        estimated_ratio = target_count / original_count if original_count > 0 else 0.5
        
        # Allow very small ratios for enhanced detail preservation
        return max(0.0005, min(0.95, estimated_ratio))
        
    except Exception as e:
        logger.warning(f"âš ï¸ Could not estimate ratio: {e}")
        return 0.5


def validate_args(args):
    """Validate command line arguments"""
    if not os.path.exists(args.input):
        print(f"Error: Input path '{args.input}' does not exist")
        sys.exit(1)
    
    if args.count and args.ratio:
        print("Error: Cannot specify both --count and --ratio")
        sys.exit(1)
    
    if not args.count and not args.ratio:
        print("Error: Must specify either --count or --ratio")
        sys.exit(1)
    
    if args.ratio and (args.ratio <= 0 or args.ratio >= 1):
        print("Error: --ratio must be between 0 and 1")
        sys.exit(1)
    
    if args.count and args.count <= 0:
        print("Error: --count must be positive")
        sys.exit(1)
    
    if args.workers <= 0:
        print("Error: --workers must be positive")
        sys.exit(1)
    
    valid_methods = ['poisson', 'ball_pivoting', 'alpha_shapes', 'none']
    if args.method not in valid_methods:
        print(f"Error: --method must be one of {valid_methods}")
        sys.exit(1)


def print_config(args, target_ratio: float, log_file: Optional[str] = None):
    """Print processing configuration"""
    logging.info("=" * 60)
    logging.info("ðŸ—¿ Enhanced Ballast Quality-Focused Reduction System v2.4.0")
    logging.info("   NEW: Adaptive Target Finding!")
    logging.info("=" * 60)
    logging.info(f"ðŸ“‚ Input: {args.input}")
    logging.info(f"ðŸ“ Output: {args.output}")
    
    if args.count:
        logging.info(f"ðŸŽ¯ Target: {args.count} points (estimated ratio: {target_ratio:.6f})")
    else:
        logging.info(f"ðŸŽ¯ Target: {args.ratio:.1%} of original points")
    
    logging.info(f"ðŸ‘¥ Workers: {args.workers}")
    logging.info(f"ðŸ”§ Method: {args.method}")
    
    features = ["Enhanced Detail Preservation", "Multi-scale Analysis", "Zone-based Processing"]
    if args.adaptive_target:
        features.append("NEW: Adaptive Target Finding")
    if args.enable_smoothing:
        features.append("NEW: Mesh Smoothing & Hole Filling")
    if args.fast_mode:
        features.append("Fast Mode")
    if args.use_random_forest:
        features.append("RandomForest")
    if args.enable_hierarchy and not args.no_hierarchy:
        features.append(f"Hierarchical (>{args.hierarchy_threshold:,} points)")
    if args.force_hierarchy:
        features.append("Force Hierarchy")
    
    logging.info(f"âœ¨ Features: {', '.join(features)}")
    
    if args.voxel:
        logging.info(f"ðŸ“¦ Voxel size: {args.voxel}")
    
    if args.verbose:
        logging.info("ðŸ” Verbose mode: ON")
    
    if log_file:
        logging.info(f"ðŸ“ Log file: {log_file}")
    
    logging.info("-" * 60)


def process_files(args, target_ratio: float) -> bool:
    """Main processing function with enhanced detail preservation and adaptive targeting"""
    
    logging.info("ðŸ—¿ Initializing Enhanced Ballast Quality-Focused Reducer v2.4.0...")
    logging.info("ðŸŽ¯ NEW: Adaptive Target Finding System ready!")
    
    # Initialize enhanced quality-focused reducer with adaptive targeting and smoothing
    reducer = BallastQualityFocusedReducer(
        target_reduction_ratio=target_ratio,
        voxel_size=args.voxel,
        n_cores=args.workers,
        reconstruction_method=args.method,
        fast_mode=args.fast_mode,
        use_random_forest=args.use_random_forest,
        enable_hierarchy=args.enable_hierarchy and not args.no_hierarchy,
        force_hierarchy=args.force_hierarchy,
        hierarchy_threshold=args.hierarchy_threshold,
        adaptive_target=args.adaptive_target,
        enable_smoothing=args.enable_smoothing  # NEW
    )
    
    adaptive_status = "ENABLED" if args.adaptive_target else "DISABLED"
    smoothing_status = "ENABLED" if args.enable_smoothing else "DISABLED"
    logging.info(f"âœ… Enhanced ballast quality-focused reducer v2.4.0 initialized successfully")
    logging.info(f"ðŸŽ¯ Adaptive target finding: {adaptive_status}")
    logging.info(f"ðŸŽ¨ Mesh smoothing & hole filling: {smoothing_status}")
    start_time = time.time()
    
    if os.path.isfile(args.input):
        # Single file processing
        logging.info(f"ðŸ”„ Processing single file: {os.path.basename(args.input)}")
        
        results = reducer.process_single_mesh(args.input, args.output)
        
        if 'error' in results:
            logging.error(f"âŒ Processing failed: {results['error']}")
            return False
        else:
            method = results.get('method_info', {}).get('processing_method', 'unknown')
            ballast_detected = results.get('ballast_detected', False)
            quality_focused = results.get('quality_focused', False)
            adaptive_enabled = results.get('adaptive_target_enabled', False)
            
            logging.info(f"âœ… SUCCESS: {results['original_points']:,} â†’ {results['final_points']:,} points")
            logging.info(f"ðŸ“Š Actual ratio: {results['reduction_ratio']:.4f}")
            logging.info(f"ðŸš€ Method used: {method}")
            if ballast_detected:
                logging.info(f"ðŸ—¿ Enhanced ballast processing: Applied")
                method_info = results.get('method_info', {})
                detail_features = method_info.get('detail_preservation_features', {})
                
                # Show adaptive target info
                if adaptive_enabled and 'adaptive_target_info' in method_info:
                    adaptive_info = method_info['adaptive_target_info']
                    logging.info(f"ðŸŽ¯ NEW: Adaptive target finding: Applied")
                    logging.info(f"   Strategy: {adaptive_info['strategy']}")
                    if 'adjustment_factor' in adaptive_info:
                        logging.info(f"   Adjustment: {adaptive_info['adjustment_factor']:.1f}x user target")
                    logging.info(f"   Reason: {adaptive_info['reason']}")
                
                if detail_features.get('multi_scale_analysis'):
                    logging.info(f"   âœ… Multi-scale surface analysis")
                if detail_features.get('zone_based_processing'):
                    logging.info(f"   âœ… Zone-based adaptive processing")
                if detail_features.get('enhanced_8_features'):
                    logging.info(f"   âœ… Enhanced 8-feature extraction")
                if detail_features.get('texture_preserving_reconstruction'):
                    logging.info(f"   âœ… Texture-preserving reconstruction")
            
    else:
        # Batch processing
        stl_files = list(Path(args.input).glob("*.stl"))
        if not stl_files:
            logging.error(f"âŒ No STL files found in {args.input}")
            return False
        
        logging.info(f"ðŸ”„ Processing {len(stl_files)} files...")
        
        results = reducer.process_batch(args.input, args.output, "*.stl")
        
        # Print summary
        successful = [r for r in results if 'error' not in r]
        failed = [r for r in results if 'error' in r]
        
        logging.info("")
        logging.info("ðŸ“Š FINAL BATCH RESULTS:")
        logging.info("=" * 40)
        logging.info(f"âœ… Successful: {len(successful)}")
        logging.info(f"âŒ Failed: {len(failed)}")
        
        if successful:
            total_original = sum(r['original_points'] for r in successful)
            total_final = sum(r['final_points'] for r in successful)
            avg_ratio = total_final / total_original if total_original > 0 else 0
            
            # Calculate vertex statistics if available
            vertex_results = [r for r in successful if 'final_vertices' in r]
            if vertex_results:
                total_original_vertices = sum(r['original_points'] for r in vertex_results)  # Using points as original vertices
                total_final_vertices = sum(r['final_vertices'] for r in vertex_results)
                avg_vertex_ratio = total_final_vertices / total_original_vertices if total_original_vertices > 0 else 0
                
                logging.info(f"ðŸ“ˆ Total points: {total_original:,} â†’ {total_final:,}")
                logging.info(f"ðŸ“ Total vertices: {total_original_vertices:,} â†’ {total_final_vertices:,}")
                logging.info(f"ðŸ“Š Average point ratio: {avg_ratio:.4f}")
                logging.info(f"ðŸ”¢ Average vertex ratio: {avg_vertex_ratio:.4f}")
                logging.info(f"ðŸŽ¯ Target ratio: {target_ratio:.4f}")
                
                # Vertex prediction accuracy for ballast models
                ballast_with_predictions = [r for r in successful if 'vertex_prediction_accuracy' in r]
                if ballast_with_predictions:
                    avg_prediction_accuracy = np.mean([r['vertex_prediction_accuracy'] for r in ballast_with_predictions])
                    logging.info(f"ðŸ”® Average vertex prediction accuracy: {avg_prediction_accuracy:.1f}%")
            else:
                logging.info(f"ðŸ“ˆ Total points: {total_original:,} â†’ {total_final:,}")
                logging.info(f"ðŸ“Š Average ratio: {avg_ratio:.4f}")
                logging.info(f"ðŸŽ¯ Target ratio: {target_ratio:.4f}")
            
            # Enhanced quality-focused analysis
            ballast_results = [r for r in successful if r.get('ballast_detected', False)]
            adaptive_results = [r for r in successful if r.get('adaptive_target_enabled', False)]
            standard_results = [r for r in successful if not r.get('ballast_detected', False)]
            
            if ballast_results:
                avg_ballast_time = np.mean([r['processing_time'] for r in ballast_results])
                avg_ballast_ratio = np.mean([r['reduction_ratio'] for r in ballast_results])
                logging.info(f"ðŸ—¿ Enhanced ballast processing: {len(ballast_results)} files")
                logging.info(f"   Average time: {avg_ballast_time:.1f}s, Average ratio: {avg_ballast_ratio:.4f}")
                
                # Vertex statistics for ballast models
                ballast_with_vertices = [r for r in ballast_results if 'final_vertices' in r]
                if ballast_with_vertices:
                    avg_vertex_reduction = np.mean([(1 - r['vertex_reduction_ratio']) * 100 for r in ballast_with_vertices])
                    logging.info(f"   Average vertex reduction: {avg_vertex_reduction:.1f}%")
            
            if adaptive_results:
                avg_adaptive_time = np.mean([r['processing_time'] for r in adaptive_results])
                logging.info(f"ðŸŽ¯ NEW: Adaptive target finding: {len(adaptive_results)} files")
                logging.info(f"   Average time: {avg_adaptive_time:.1f}s")
            
            if standard_results:
                avg_standard_time = np.mean([r['processing_time'] for r in standard_results])
                logging.info(f"ðŸ“ Standard processing: {len(standard_results)} files, avg {avg_standard_time:.1f}s")
        
        if failed and len(failed) <= 10:
            logging.warning(f"âŒ Failed files:")
            for result in failed:
                filename = os.path.basename(result.get('input_file', 'unknown'))
                error = result.get('error', 'Unknown error')
                logging.warning(f"   â€¢ {filename}: {error}")
        elif failed:
            logging.warning(f"âŒ {len(failed)} files failed (check batch_summary.csv for details)")
    
    return True


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description="Enhanced Ballast Quality-Focused Point Cloud Reducer v2.4.0 (Adaptive Target Finding)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples (SAME COMMANDS - now with ADAPTIVE TARGET FINDING + MESH SMOOTHING!):
  python ballast-quality-focused-v2.4.0.py /home/railcmu/Desktop/BPK/ballast --count 100 --workers 2 --adaptive-target
  python ballast-quality-focused-v2.4.0.py /path/to/models --ratio 0.3 --workers 8 --adaptive-target --enable-smoothing
  python ballast-quality-focused-v2.4.0.py model.stl --count 50 --method poisson --adaptive-target --disable-smoothing

ðŸŽ¯ NEW in v2.4.0 - ADAPTIVE TARGET FINDING + VERTEX PREDICTION:
  --adaptive-target          Enable automatic optimal target detection when user target fails
  
  ADAPTIVE TARGET STRATEGIES:
  â€¢ Conservative: Prioritizes reconstruction success (2x-5x user target)
  â€¢ Moderate: Balanced approach (1.5x-3x user target)  
  â€¢ Aggressive: Stays closer to user target (1.2x-2.5x user target)
  
  AUTOMATIC FEATURES:
  â€¢ Multi-strategy target adaptation based on model complexity
  â€¢ Reconstruction-aware target adjustment with fallback mechanisms
  â€¢ Progressive target scaling with quality validation
  â€¢ Smart retry logic with complexity-based target ranges
  â€¢ Emergency fallback to safe targets for guaranteed reconstruction success
  
  ðŸ“Š NEW: COMPREHENSIVE VERTEX CALCULATION & PREDICTION:
  â€¢ Predicts final vertex counts before processing starts
  â€¢ Estimates memory usage for different reconstruction methods
  â€¢ Calculates vertex reduction ratios and compression factors
  â€¢ Tracks vertex density and mesh quality metrics
  â€¢ Provides prediction accuracy analysis
  â€¢ Memory usage optimization recommendations
  
  ðŸŽ¨ NEW: MESH SMOOTHING & HOLE FILLING SYSTEM:
  â€¢ Automatic hole detection and repair for watertight meshes
  â€¢ Multiple smoothing algorithms (Laplacian, Taubin, Simple)
  â€¢ Adaptive reconstruction for smooth, hole-free surfaces
  â€¢ Quality validation to ensure perfect mesh integrity
  â€¢ Conservative to aggressive hole filling strategies
  â€¢ Post-reconstruction optimization for smooth surfaces

ENHANCED Detail-Preserving + Target-Respecting Features:
  - Multi-scale surface complexity analysis for better detail detection
  - Zone-based adaptive processing (high/medium/low detail areas)
  - Enhanced 8-feature extraction for rough surfaces
  - Texture-preserving reconstruction methods (Alpha Shapes, Ball Pivoting)
  - Multi-pass importance detection with enhanced edge preservation
  - Conservative clustering with detail zones
  - Much better boundary and edge detection
  - NOW: Automatic target finding when user target fails
  - NOW: Comprehensive vertex prediction and calculation system
  - Respects user targets while maximizing ballast quality and ensuring success

Performance Features:
  --fast-mode                 Skip parameter optimization
  --use-random-forest        Use RandomForest instead of SVM
  --no-hierarchy             Disable automatic hierarchical processing
  --force-hierarchy          Force hierarchical processing on all models
  --hierarchy-threshold N     Set threshold for hierarchical processing

What's NEW in v2.4.0:
ðŸŽ¯ Adaptive Target Finding System:
âœ… Automatic optimal target detection when initial target fails
âœ… Multi-strategy adaptation (conservative/moderate/aggressive)
âœ… Reconstruction-aware target adjustment with retry logic
âœ… Progressive target scaling with quality validation
âœ… Smart complexity-based target ranges
âœ… Emergency fallback mechanisms for guaranteed success
âœ… SAME interface - just add --adaptive-target flag!

ðŸ“Š Comprehensive Vertex Calculation & Prediction System:
âœ… Pre-processing vertex count prediction for all reconstruction methods
âœ… Real-time vertex estimation based on surface complexity
âœ… Memory usage calculation and optimization recommendations
âœ… Vertex reduction ratio and compression factor tracking
âœ… Vertex density and mesh quality analysis
âœ… Prediction accuracy validation (actual vs predicted)
âœ… Comprehensive vertex statistics in batch summaries
âœ… Performance impact analysis for different vertex densities

ðŸŽ¨ Mesh Smoothing & Hole Filling System:
âœ… Automatic hole detection and quality analysis
âœ… Multiple hole filling strategies (conservative/moderate/aggressive)
âœ… Advanced smoothing algorithms (Laplacian, Taubin, Simple)
âœ… Adaptive smooth reconstruction with quality validation
âœ… Post-reconstruction mesh optimization pipeline
âœ… Watertight mesh guarantee with quality scoring
âœ… Perfect surface generation for smooth reduced models
âœ… Boundary edge detection and repair capabilities

Previous Features (Still Included):
âœ… Multi-scale surface analysis for complex ballast textures
âœ… Zone-based processing (high/medium/low complexity areas)
âœ… Enhanced 8-feature extraction (vs 6 before)
âœ… Texture-preserving reconstruction (Alpha Shapes primary method)
âœ… Much more conservative clustering preserves detail
âœ… Better boundary and edge detection with convex hull
âœ… Multi-pass importance detection with zone weighting

Expected Results with Adaptive Targeting + Vertex Prediction + Mesh Smoothing:
- Significantly higher success rate for difficult targets
- Automatic fallback when user target is too aggressive
- Better balance between user intent and reconstruction feasibility
- Accurate prediction of final mesh complexity before processing
- Memory usage optimization based on vertex predictions
- PERFECT SMOOTH SURFACES with no holes or discontinuities
- Watertight meshes guaranteed through advanced hole filling
- Much better preservation of surface texture and roughness
- Better retention of edges, corners, and surface features
- Eliminated mesh artifacts and surface irregularities
- Professional-quality smooth reduced meshes suitable for all applications
- More accurate representation of ballast geometry
- Guaranteed successful reconstruction (with adaptive fallbacks)
- Comprehensive vertex and quality analysis for optimal results

Installation:
  pip install numpy pandas scikit-learn trimesh open3d
  # Optional for enhanced boundary detection: pip install scipy
        """
    )
    
    # Arguments
    parser.add_argument('input', 
                       help='Input STL file or directory containing STL files')
    
    target_group = parser.add_mutually_exclusive_group(required=True)
    target_group.add_argument('--count', type=int,
                             help='Target number of points to keep')
    target_group.add_argument('--ratio', type=float,
                             help='Target reduction ratio (0.0-1.0)')
    
    parser.add_argument('--workers', type=int, default=4,
                       help='Number of parallel workers (default: 4)')
    parser.add_argument('--output', type=str, default='output',
                       help='Output directory (default: output)')
    
    parser.add_argument('--method', type=str, default='alpha_shapes',
                       choices=['alpha_shapes', 'ball_pivoting', 'poisson', 'none'],
                       help='Surface reconstruction method (default: alpha_shapes for detail)')
    parser.add_argument('--voxel', type=float,
                       help='Voxel size for preprocessing downsampling')
    
    # NEW: Adaptive target finding and mesh smoothing
    adaptive_group = parser.add_argument_group('adaptive target finding & mesh smoothing (NEW in v2.4.0)')
    adaptive_group.add_argument('--adaptive-target', action='store_true',
                               help='ðŸŽ¯ NEW: Enable adaptive target finding when user target fails')
    adaptive_group.add_argument('--enable-smoothing', action='store_true', default=True,
                               help='ðŸŽ¨ NEW: Enable mesh smoothing and hole filling for perfect surfaces (default: enabled)')
    adaptive_group.add_argument('--disable-smoothing', action='store_true',
                               help='ðŸš« Disable mesh smoothing and hole filling (faster but may have holes)')
    
    # Performance options
    performance_group = parser.add_argument_group('performance options')
    performance_group.add_argument('--fast-mode', action='store_true',
                                 help='Skip parameter optimization for faster processing')
    performance_group.add_argument('--use-random-forest', action='store_true', default=True,
                                 help='Use RandomForest classifier (default, faster than SVM)')
    performance_group.add_argument('--use-svm', action='store_true',
                                 help='Use SVM classifier (slower but potentially higher quality)')
    
    # Hierarchical processing options
    hierarchy_group = parser.add_argument_group('hierarchical processing options')
    hierarchy_group.add_argument('--no-hierarchy', action='store_true',
                               help='Disable automatic hierarchical processing')
    hierarchy_group.add_argument('--force-hierarchy', action='store_true',
                               help='Force hierarchical processing on all models')
    hierarchy_group.add_argument('--hierarchy-threshold', type=int, default=50000,
                               help='Point count threshold for hierarchical processing (default: 50000)')
    
    # Utility options
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose logging')
    parser.add_argument('--log-file', type=str,
                       help='Custom log file path (default: auto-generated in logs/ folder)')
    parser.add_argument('--no-log', action='store_true',
                       help='Disable automatic log file creation (console only)')
    parser.add_argument('--version', action='version', version='2.4.0 (Adaptive Target Finding + Vertex Prediction + Mesh Smoothing & Hole Filling + Enhanced Detail-Preserving)')
    
    args = parser.parse_args()
    
    # Handle classifier selection
    if args.use_svm:
        args.use_random_forest = False
    
    # Handle smoothing options
    if args.disable_smoothing:
        args.enable_smoothing = False
    
    # Enable hierarchy by default unless disabled
    args.enable_hierarchy = not args.no_hierarchy
    
    # Auto-generate log file path if not specified and not disabled
    log_file = None
    if not getattr(args, 'no_log', False):
        if getattr(args, 'log_file', None):
            log_file = args.log_file
        else:
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            input_name = Path(args.input).name if os.path.isfile(args.input) else Path(args.input).name
            adaptive_suffix = "_adaptive" if args.adaptive_target else ""
            log_file = f"logs/{input_name}_enhanced_ballast_v2.4.0{adaptive_suffix}_{timestamp}.log"
    
    # Setup logging
    logger = setup_logging(args.verbose, log_file)
    
    # Log startup information
    logging.info("ðŸ—¿ Enhanced Ballast Quality-Focused Point Cloud Reduction System v2.4.0 Starting...")
    logging.info("ðŸŽ¯ NEW FEATURE: Adaptive Target Finding System!")
    logging.info("ðŸ“Š NEW FEATURE: Comprehensive Vertex Prediction & Calculation!")
    logging.info(f"ðŸ“… Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logging.info(f"ðŸ’» System: Python {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}")
    logging.info(f"âš¡ Available CPU cores: {mp.cpu_count()}")
    
    # Validate arguments
    validate_args(args)
    
    # Calculate target ratio
    if args.count:
        logging.info(f"ðŸŽ¯ Estimating enhanced target ratio for {args.count} points...")
        target_ratio = estimate_target_ratio(args.input, args.count)
        logging.info(f"ðŸ“Š Enhanced estimated target ratio: {target_ratio:.6f}")
    else:
        target_ratio = args.ratio
        logging.info(f"ðŸ“Š Using specified ratio: {target_ratio:.6f}")
    
    # Create output directory
    os.makedirs(args.output, exist_ok=True)
    logging.info(f"ðŸ“ Output directory ready: {args.output}")
    
    # Print configuration
    print_config(args, target_ratio, log_file)
    
    # Process files
    try:
        logging.info("ðŸ Starting enhanced ballast quality-focused processing with adaptive targeting...")
        success = process_files(args, target_ratio)
        
        if success:
            logging.info("")
            logging.info("ðŸŽ‰ PROCESSING COMPLETED SUCCESSFULLY!")
            logging.info("ðŸ—¿ Enhanced Ballast Quality-Focused Reduction v2.4.0 - Adaptive Target Finding + Vertex Prediction!")
            logging.info(f"ðŸ• Finished at: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            logging.info("âœ… ENHANCED FEATURES APPLIED:")
            logging.info("   â€¢ Multi-scale surface analysis for better detail detection")
            logging.info("   â€¢ Zone-based adaptive processing (high/medium/low detail)")
            logging.info("   â€¢ Enhanced 8-feature extraction for rough surfaces")
            logging.info("   â€¢ Texture-preserving reconstruction methods")
            logging.info("   â€¢ Much better boundary and edge preservation")
            if args.adaptive_target:
                logging.info("   â€¢ ðŸŽ¯ NEW: Adaptive target finding for guaranteed success")
            logging.info("   â€¢ ðŸ“Š NEW: Comprehensive vertex prediction & calculation")
            if args.enable_smoothing:
                logging.info("   â€¢ ðŸŽ¨ NEW: Mesh smoothing & hole filling for perfect surfaces")
            if log_file:
                logging.info(f"ðŸ“ Detailed logs saved to: {log_file}")
            sys.exit(0)
        else:
            logging.error("")
            logging.error("ðŸ’¥ PROCESSING FAILED!")
            logging.error("Check the error messages above for details")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logging.warning("")
        logging.warning("â¹ï¸  Processing interrupted by user (Ctrl+C)")
        logging.warning("Partial results may be available in output directory")
        sys.exit(1)
    except Exception as e:
        logging.error("")
        logging.error(f"ðŸ’¥ Unexpected error: {e}")
        if args.verbose:
            import traceback
            logging.error("ðŸ” Full traceback:")
            logging.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main()
